\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{nb-lesson-3-2}
    
    
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`$}{\char`$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \section{Supervised Models: Perceptrons and Support Vector Machines
(SVM)}\label{supervised-models-perceptrons-and-support-vector-machines-svm}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k+kn}{import} \PY{n}{Image}
\PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\end{Verbatim}
\end{tcolorbox}

    \subsection{Concept of Linearly Separable
Dataset}\label{concept-of-linearly-separable-dataset}

A \textbf{linearly separable dataset} is one in which the data points
from two different classes can be perfectly separated by a straight line
(in 2D), or a hyperplane (in higher dimensions). In simpler terms, there
exists a linear decision boundary that can divide the dataset into two
distinct groups, where each group corresponds to one of the classes.

    \subsubsection{What is a Hyperplane?}\label{what-is-a-hyperplane}

In geometry, it is an n-dimensional generalization of a plane, a
subspace with one less dimension $(n-1)$ than its origin space. In
one-dimensional space, it is a point, In two-dimensional space it is a
line, In three-dimensional space, it is an ordinary plane, in four or
more dimensional spaces, it is then called a `Hyperplane'. Take note of
this, it is really how the Support Vector Machine works behind the
scenes, the dimensions are the features represented in the data. For
example, say we want to carry out a classification problem and we want
to be able to tell if a product gets purchased or not(a binary
classification), if there is just one feature (say Gender) available as
a feature in the dataset, then it is in one-dimensional space and the
subspace (the separating/decision boundary) representation will be
$(n-1=0)$ a 0-dimensional space, represented with just a point showing
the separation of classes (Purchased or not). If there are two
features(Age and Gender), it is a two-dimensional space (2D), with
either of Age and Gender on the $X$ and $Y$-axis, the decision
boundary will be represented as a simple line. Similarly, if the
features are three(Age, Gender, Income), the decision boundary will be a
two-dimensional plane in a three-dimensional space. Furthermore, if we
have a four or more dimensional space data points, then it is called a
`Hyperplane' with $n-1$ dimension.

The Hyperplane is simply a concept that separates an n-dimensional space
into two groups/halves. In machine learning terms, it is a form of a
decision boundary that algorithms like the Support Vector Machine uses
to classify or separate data points. There are two parts to it, the
negative side hyperplane and the positive part hyperplane, where data
points/instances can lie on either part, signifying the group/class they
belong to.

    

    \subsubsection{Key Characteristics:}\label{key-characteristics}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{2D Case}: In a two-dimensional dataset, the two classes can be
  separated by a straight line.
\item
  \textbf{Line Equation}: The separating line can be defined by the
  equation:

  $$
  w_1 x_1 + w_2 x_2 + b = 0
  $$
\end{enumerate}

Where: - $x_1$ and $x_2$ are the features of the data points. -
$w_1$ and $w_2$ are the weights (coefficients) that define the
orientation of the line. - $b$ is the bias, which shifts the line
vertically.

\subsubsection{Problem Definition}\label{problem-definition}

Consider a simple 2D problem where each data point has two features
($x_1$ and $x_2$), and we want to classify them into two categories
(let's call them Class 1 and Class 2). A line can be drawn to separate
the data points of Class 1 from those of Class 2.

\textbf{Example Dataset:}

\begin{itemize}
\tightlist
\item
  Class 1: Points that lie on one side of the line (e.g., above the
  line).
\item
  Class 2: Points that lie on the other side of the line (e.g., below
  the line).
\end{itemize}

\subsubsection{Visualizing the Separating
Line}\label{visualizing-the-separating-line}

For a 2D dataset, the equation of the line can be written as:

$$
x_2 = -\frac{w_1}{w_2} x_1 - \frac{b}{w_2}
$$

This equation will give the corresponding $x_2$ values (the vertical
axis) for any given $x_1$ values (the horizontal axis). We can plot
this line and the data points to visually verify the separability.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}

\PY{c+c1}{\PYZsh{} Create a simple linearly separable dataset (2D)}
\PY{c+c1}{\PYZsh{} Class 1: points above the line y = x}
\PY{c+c1}{\PYZsh{} Class 2: points below the line y = x}
\PY{n}{X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{]}\PY{p}{]}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Class 1}
\PY{n}{y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Labels for Class 1}

\PY{n}{X2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{]}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Class 2}
\PY{n}{y2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Labels for Class 2}

\PY{n}{X\PYZus{}combined} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{[}\PY{n}{X}\PY{p}{,} \PY{n}{X2}\PY{p}{]}\PY{p}{)}
\PY{n}{y\PYZus{}combined} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{y}\PY{p}{,} \PY{n}{y2}\PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Visualize the data}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X2}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X2}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class 2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Create a separating line y = x}
\PY{n}{x\PYZus{}values} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}
\PY{n}{y\PYZus{}values} \PY{o}{=} \PY{n}{x\PYZus{}values}  \PY{c+c1}{\PYZsh{} y = x for the line}

\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}values}\PY{p}{,} \PY{n}{y\PYZus{}values}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Separating Line}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nb-lesson-3-2_files/nb-lesson-3-2_6_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Calculating the Separating
Line}\label{calculating-the-separating-line}

To calculate the line that separates the two classes, we can use the
\textbf{Perceptron algorithm} or another classifier to find the optimal
weights $w_1$, $w_2$, and bias $b$ that define the separating
line.

    \subsubsection{\texorpdfstring{Geometrical Interpretation of the Vector
$W$}{Geometrical Interpretation of the Vector W}}\label{geometrical-interpretation-of-the-vector-w}

the set of weights $(w_1, w_2)$ is nothing more than a vector $W$.
But what exactly does $W$ represent? We can easily verify that the
vector $\mathbf{w} = (w_1, w_2)$ is perpendicular to the line
$w_1 x_1 + w_2 x_2 + b = 0$. To demonstrate this relationship we can
use the following argument based on the properties of the dot product.

\textbf{Step 1: Consider Two Points on the Line}

Let $\mathbf{x} = (x_1, x_2)$ and $\mathbf{y} = (y_1, y_2)$ be any
two distinct points on the line. Because both points lie on the line,
they satisfy the equation:

$$
w_1 x_1 + w_2 x_2 + b = 0 \quad \text{and} \quad w_1 y_1 + w_2 y_2 + b = 0.
$$

\textbf{Step 2: Form a Tangent Vector to the Line}

A vector that is tangent to the line can be obtained by taking the
difference between the two points:

$$
\mathbf{v} = \mathbf{y} - \mathbf{x} = (y_1 - x_1, \, y_2 - x_2).
$$

\textbf{Step 3: Show That $\mathbf{w}$ is Perpendicular to
$\mathbf{v}$}

To show that $\mathbf{w}$ is perpendicular to the tangent vector
$\mathbf{v}$, we need to demonstrate that their dot product is zero:

$$
\mathbf{w} \cdot \mathbf{v} = w_1 (y_1 - x_1) + w_2 (y_2 - x_2).
$$

\textbf{Step 4: Use the Line Equations}

Since both $\mathbf{x}$ and $\mathbf{y}$ lie on the line, we have:

$$
w_1 x_1 + w_2 x_2 + b = 0 \quad \Rightarrow \quad w_1 x_1 + w_2 x_2 = -b,
$$

$$
w_1 y_1 + w_2 y_2 + b = 0 \quad \Rightarrow \quad w_1 y_1 + w_2 y_2 = -b.
$$

Subtract the first equation from the second:

$$
(w_1 y_1 + w_2 y_2) - (w_1 x_1 + w_2 x_2) = -b - (-b) = 0.
$$

This simplifies to:

$$
w_1 (y_1 - x_1) + w_2 (y_2 - x_2) = 0.
$$

\textbf{Step 5: Conclude Perpendicularity}

Since

$$
\mathbf{w} \cdot \mathbf{v} = 0,
$$

the vector $\mathbf{w} = (w_1, w_2)$ is orthogonal to the tangent
vector $\mathbf{v}$ of the line. Because $\mathbf{v}$ is an
arbitrary tangent vector to the line, this proves that $\mathbf{w}$ is
perpendicular to the separating line.

    \subsubsection{Describing Points Above or Below the
Line}\label{describing-points-above-or-below-the-line}

Because the vector $ \mathbf{w} $ is perpendicular to the line, we can
use it to ``step off'' the line in a controlled way. Specifically,
consider any point of the form

$$
\mathbf{x} = \mathbf{x}_0 + \alpha\, \mathbf{u},
$$

where: - $\mathbf{x}_0$ is a point on the line (so that
$w_1 x_{1,0} + w_2 x_{2,0} + b = 0$), - $\mathbf{u}$ is a unit
vector in the direction of $ \mathbf{w} $, i.e.,

$$
  \mathbf{u} = \frac{(w_1, w_2)}{\|\mathbf{w}\|},
  $$

\begin{itemize}
\tightlist
\item
  and $\alpha$ is a scalar.
\end{itemize}

Now, depending on the sign of $\alpha$, the point $\mathbf{x}$ will
lie on one side of the line or the other: - If $\alpha > 0$, then
$\mathbf{x}$ lies in the direction of $ \mathbf{w} $ (which we will
call ``above'' the line). - If $\alpha < 0$, then $\mathbf{x}$ lies
in the direction opposite to $ \mathbf{w} $ (which we will call
``below'' the line).

\textbf{Evaluating the Linear Combination}

Let's compute the value of

$$
f(\mathbf{x}) = w_1 x_1 + w_2 x_2 + b,
$$

for the point

$$
\mathbf{x} = \mathbf{x}_0 + \alpha\, \mathbf{u}.
$$

Substitute $\mathbf{x}$ into $f$:

$$
\begin{aligned}
f(\mathbf{x}) &= w_1 (x_{1,0} + \alpha u_1) + w_2 (x_{2,0} + \alpha u_2) + b \\
&= \bigl(w_1 x_{1,0} + w_2 x_{2,0} + b\bigr) + \alpha \bigl(w_1 u_1 + w_2 u_2\bigr).
\end{aligned}
$$

Since $\mathbf{x}_0$ lies on the line, the first term is zero:

$$
w_1 x_{1,0} + w_2 x_{2,0} + b = 0.
$$

Now, consider the second term. Because $\mathbf{u}$ is the unit vector
in the direction of $\mathbf{w}$:

$$
w_1 u_1 + w_2 u_2 = \mathbf{w} \cdot \mathbf{u} = \|\mathbf{w}\| \, \|\mathbf{u}\| \, \cos(0) = \|\mathbf{w}\| \cdot 1 \cdot 1 = \|\mathbf{w}\|.
$$

Thus, we have:

$$
f(\mathbf{x}) = \alpha \|\mathbf{w}\|.
$$

\textbf{Interpreting the Result}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{For Points Above the Line:}

  When $\alpha > 0$, we get

  $$
  f(\mathbf{x}) = \alpha \|\mathbf{w}\| > 0,
  $$

  because $\|\mathbf{w}\|$ is a positive quantity. Hence, all points
  that are a positive distance $\alpha$ from the line (in the
  direction of $ \mathbf{w} $) yield

  $$
  w_1 x_1 + w_2 x_2 + b > 0.
  $$
\item
  \textbf{For Points Below the Line:}

  When $\alpha < 0$, we obtain

  $$
  f(\mathbf{x}) = \alpha \|\mathbf{w}\| < 0,
  $$

  so all points that are a negative distance $\alpha$ from the line
  (opposite to the direction of $ \mathbf{w} $) yield

  $$
  w_1 x_1 + w_2 x_2 + b < 0.
  $$
\end{enumerate}

This demonstration shows that by expressing any point as a shift from a
point on the decision boundary along the direction of the normal vector
$ \mathbf{w} $, we obtain a clear relationship:

\begin{itemize}
\tightlist
\item
  Points in the direction of $ \mathbf{w} $ (above the line) yield
  $w_1 x_1 + w_2 x_2 + b > 0$.
\item
  Points in the opposite direction (below the line) yield
  $w_1 x_1 + w_2 x_2 + b < 0$.
\end{itemize}

Thus, the sign of $w_1 x_1 + w_2 x_2 + b$ can be used to correctly
classifies points relative to the separating line.

    \subsubsection{How to Classify a New Point Given the Separating
Line}\label{how-to-classify-a-new-point-given-the-separating-line}

Given a linearly separable dataset and a separating line $ w_1 x_1 +
w_2 x_2 + b = 0 $, we can classify a new point by calculating the
value of the linear combination $ w_1 x_1 + w_2 x_2 + b $. Here's
how the classification works:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Calculate the Linear Combination}:

  For a new point $\vec{x}= (x_1, x_2) $, compute the value of:

  $$
  f(\vec{x}) = w_1 x_1 + w_2 x_2 + b
  $$

  This gives a scalar value.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Determine the Class}:

  \begin{itemize}
  \tightlist
  \item
    If the linear combination is greater than or equal to 0 ($ \geq 0
    $), we classify the point as belonging to \textbf{Class 1} (above
    the line).
  \item
    If the linear combination is less than 0 ($ \textless{} 0 $), we
    classify the point as belonging to \textbf{Class 2} (below the
    line).
  \end{itemize}
\end{enumerate}

The separating line is essentially the decision boundary. Any point on
the line will have a linear combination equal to 0, while points on
either side of the line will yield positive or negative values,
indicating which class the point belongs to.

\textbf{Example}

Let's assume the following values for the separating line: - $ w_1 = 1
$ - $ w_2 = -1 $ - $ b = 0.5 $

We will now classify two new points: - Point A: $ (x_1 = 3, x_2 = 2)
$ (above the line) - Point B: $ (x_1 = 2, x_2 = 3) $ (below the
line)

\textbf{Step 1: Calculate the Linear Combination for Point A}

For point A ($ x_1 = 3, x_2 = 2 $):

$$
w_1 x_1 + w_2 x_2 + b = (1 \times 3) + (-1 \times 2) + 0.5 = 3 - 2 + 0.5 = 1.5
$$

Since the linear combination is \textbf{greater than 0} ($ 1.5 \geq 0
$), point A is classified as \textbf{Class 1} (above the line).

\textbf{Step 2: Calculate the Linear Combination for Point B}

For point B ($ x_1 = 2, x_2 = 3 $):

$$
w_1 x_1 + w_2 x_2 + b = (1 \times 2) + (-1 \times 3) + 0.5 = 2 - 3 + 0.5 = -0.5
$$

Since the linear combination is \textbf{less than 0} ($ -0.5
\textless{} 0 $), point B is classified as \textbf{Class 2} (below the
line).

\textbf{Summary of Results:}

\begin{itemize}
\tightlist
\item
  Point A (3, 2): The linear combination is 1.5, so the point is
  classified as \textbf{Class 1} (above the line).
\item
  Point B (2, 3): The linear combination is -0.5, so the point is
  classified as \textbf{Class 2} (below the line).
\end{itemize}

This process of calculating the linear combination and using it to
classify a point is at the core of how a perceptron or linear classifier
works. The sign of the linear combination determines the class of the
point relative to the separating line.

    \subsection{The Perceptron}\label{the-perceptron}

    The \textbf{Perceptron} is one of the simplest types of artificial
neural networks and is considered a foundational algorithm in machine
learning and neural network theory. It was introduced by Frank
Rosenblatt in 1958 and is primarily used for binary classification
tasks, i.e., determining whether an input belongs to one of two classes.

A Perceptron works by classifying input vectors through a linear
decision boundary, which is adjusted during the training phase to
minimize classification errors.

\subsubsection{Structure of a
Perceptron:}\label{structure-of-a-perceptron}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Input layer}: The perceptron has multiple inputs, where each
  input $ x_i $ represents a feature of the data.
\item
  \textbf{Weights}: Each input has an associated weight $ w_i $. The
  weight determines the importance of the corresponding input feature in
  the classification decision.
\item
  \textbf{Bias}: A bias term $ b $ is used to shift the decision
  boundary and provide more flexibility in the classification.
\item
  \textbf{Activation Function}: The perceptron uses a step activation
  function, which produces an output of either 0 or 1 based on whether
  the weighted sum of inputs exceeds a certain threshold.
\end{enumerate}

The mathematical formulation of the perceptron is as follows:

$$
y = \begin{cases} 
1 & \text{if } \sum\limits_{i=1}^{n} w_i x_i + b \geq 0 \\
&\\
0 & \text{if } \sum\limits_{i=1}^{n} w_i x_i + b < 0
\end{cases}
$$

Where: - $ x_i $ are the input features - $ w_i $ are the weights
- $ b $ is the bias - $ y $ is the output class (1 or 0)

The output is binary, meaning that it will predict one of two classes.

\subsubsection{Training the Perceptron:}\label{training-the-perceptron}

The perceptron is trained using a \textbf{supervised learning}
algorithm. The goal of training is to adjust the weights and bias to
correctly classify the input data. The Perceptron uses the following
update rule during training:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize the weights and bias to small random values.
\item
  For each training sample $ (x^i, y^i) $:

  \begin{itemize}
  \item
    Calculate the predicted output $ \hat{y}^i = \text{sign}(w
    \cdot x^i + b) $
  \item
    If $ \hat{y}^i \neq y^i $, update the weights
    and bias:

    $$ w_i \leftarrow w_i + \eta (y^{(i)} - \hat{y}^{(i)}) x_i^{(i)} $$

    $$ b \leftarrow b + \eta (y^{(i)} - \hat{y}^{(i)}) $$
  \end{itemize}
\end{enumerate}

Where: - $ \eta $ is the learning rate - $ \hat{y}^i $ is
the predicted output for the input $ x^i $ - $ y^i
$ is the actual class for the input

\subsubsection{Convergence and
Limitations:}\label{convergence-and-limitations}

The perceptron algorithm converges to a solution if the classes are
linearly separable (i.e., there is a linear decision boundary that can
perfectly separate the two classes). If the classes are not linearly
separable, the perceptron will not converge and may fail to find a
solution.

\subsubsection{Python Example: Implementing the
Perceptron}\label{python-example-implementing-the-perceptron}

Here's an implementation of the perceptron in Python using a basic
dataset:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}blobs}

\PY{c+c1}{\PYZsh{} Generate a linearly separable dataset}
\PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{make\PYZus{}blobs}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{,} \PY{n}{centers}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{n\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,}
                  \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{123}\PY{p}{,} \PY{n}{cluster\PYZus{}std}\PY{o}{=}\PY{l+m+mf}{2.0}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Visualize the generated dataset}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class 0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Linearly Separable Dataset}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Feature 1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Feature 2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nb-lesson-3-2_files/nb-lesson-3-2_13_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{k}{class} \PY{n+nc}{Perceptron}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    A simple implementation of a Perceptron for binary classification.}

\PY{l+s+sd}{    The Perceptron is a linear classifier that uses a step function as its activation.}
\PY{l+s+sd}{    It learns a weight vector and a bias term to separate input data into two classes.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Attributes:}
\PY{l+s+sd}{        weights (numpy.ndarray): The weight vector for the input features.}
\PY{l+s+sd}{        bias (float): The bias term added to the linear combination of inputs.}
\PY{l+s+sd}{        learning\PYZus{}rate (float): The step size used during the weight update.}
\PY{l+s+sd}{        epochs (int): The number of iterations over the entire training dataset.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}size}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Initialize the Perceptron.}

\PY{l+s+sd}{        Parameters:}
\PY{l+s+sd}{            input\PYZus{}size (int): The number of features in the input data.}
\PY{l+s+sd}{            learning\PYZus{}rate (float): The learning rate for updating weights and bias.}
\PY{l+s+sd}{            epochs (int): The number of times the training algorithm will iterate over the training set.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{} Initialize the weights to a vector of zeros with length equal to input\PYZus{}size.}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{input\PYZus{}size}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Initialize the bias term to 0.}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias} \PY{o}{=} \PY{l+m+mf}{0.0}
        \PY{c+c1}{\PYZsh{} Set the learning rate (step size for updates).}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{n}{learning\PYZus{}rate}
        \PY{c+c1}{\PYZsh{} Set the number of epochs (iterations over the entire dataset).}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epochs} \PY{o}{=} \PY{n}{epochs}
    
    \PY{k}{def} \PY{n+nf}{activate}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Step activation function.}

\PY{l+s+sd}{        Parameters:}
\PY{l+s+sd}{            x (float): The input value (typically the linear combination of features and weights).}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Returns:}
\PY{l+s+sd}{            int: Returns 1 if x is greater than or equal to 0, otherwise returns 0.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k}{return} \PY{l+m+mi}{1} \PY{k}{if} \PY{n}{x} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mi}{0} \PY{k}{else} \PY{l+m+mi}{0}
    
    \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Predict the class label for a given input.}

\PY{l+s+sd}{        This method computes the linear combination of the input features and the weights,}
\PY{l+s+sd}{        adds the bias, and then applies the step activation function.}

\PY{l+s+sd}{        Parameters:}
\PY{l+s+sd}{            x (numpy.ndarray): The input vector for which to predict the class label.}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Returns:}
\PY{l+s+sd}{            int: The predicted class label (either 0 or 1).}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{} Compute the weighted sum (linear combination) of the input features plus bias.}
        \PY{n}{linear\PYZus{}output} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights}\PY{p}{)} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}
        \PY{c+c1}{\PYZsh{} Apply the step activation function to determine the class.}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{activate}\PY{p}{(}\PY{n}{linear\PYZus{}output}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{train}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Train the Perceptron on the given dataset.}

\PY{l+s+sd}{        The training process involves iterating over the dataset for a set number of epochs.}
\PY{l+s+sd}{        For each sample, the algorithm computes the prediction and updates the weights}
\PY{l+s+sd}{        and bias based on the error (difference between actual and predicted values).}

\PY{l+s+sd}{        Parameters:}
\PY{l+s+sd}{            X (numpy.ndarray): The input dataset with each row as a sample.}
\PY{l+s+sd}{            y (numpy.ndarray): The target labels corresponding to each sample in X.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{} Loop over the specified number of epochs.}
        \PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Iterate over each sample in the dataset.}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} Compute the prediction for the i\PYZhy{}th sample.}
                \PY{n}{prediction} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} Compute the error (difference between true label and prediction).}
                \PY{n}{error} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{prediction}
                \PY{c+c1}{\PYZsh{} Update the weights: add the product of learning rate, error, and input features.}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights} \PY{o}{+}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{learning\PYZus{}rate} \PY{o}{*} \PY{n}{error} \PY{o}{*} \PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]}
                \PY{c+c1}{\PYZsh{} Update the bias: add the product of learning rate and error.}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias} \PY{o}{+}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{learning\PYZus{}rate} \PY{o}{*} \PY{n}{error}
                
    \PY{k}{def} \PY{n+nf}{evaluate}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Evaluate the accuracy of the Perceptron on a given dataset.}

\PY{l+s+sd}{        This method calculates the proportion of correct predictions over the entire dataset.}

\PY{l+s+sd}{        Parameters:}
\PY{l+s+sd}{            X (numpy.ndarray): The input dataset with each row as a sample.}
\PY{l+s+sd}{            y (numpy.ndarray): The true labels for each sample in X.}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Returns:}
\PY{l+s+sd}{            float: The accuracy of the model as a fraction between 0 and 1.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{} Initialize a counter for correct predictions.}
        \PY{n}{correct\PYZus{}predictions} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{c+c1}{\PYZsh{} Iterate over each sample in the dataset.}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} If the prediction matches the true label, increment the counter.}
            \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)} \PY{o}{==} \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{:}
                \PY{n}{correct\PYZus{}predictions} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
        \PY{c+c1}{\PYZsh{} Calculate accuracy as the ratio of correct predictions to total samples.}
        \PY{n}{accuracy} \PY{o}{=} \PY{n}{correct\PYZus{}predictions} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}
        \PY{k}{return} \PY{n}{accuracy}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Initialize the perceptron with 2 inputs}
\PY{n}{perceptron} \PY{o}{=} \PY{n}{Perceptron}\PY{p}{(}\PY{n}{input\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
    
\PY{c+c1}{\PYZsh{} Train the perceptron on the dataset}
\PY{n}{perceptron}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
    
\PY{c+c1}{\PYZsh{} Evaluate the model}
\PY{n}{accuracy} \PY{o}{=} \PY{n}{perceptron}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training Accuracy: }\PY{l+s+si}{\PYZob{}}\PY{n}{accuracy}\PY{+w}{ }\PY{o}{*}\PY{+w}{ }\PY{l+m+mi}{100}\PY{l+s+si}{:}\PY{l+s+s2}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    
\PY{c+c1}{\PYZsh{} Plot the decision boundary learned by the perceptron}
\PY{c+c1}{\PYZsh{} The decision boundary is given by: w1*x1 + w2*x2 + b = 0}
\PY{c+c1}{\PYZsh{} Solve for x2: x2 = \PYZhy{}(w1/w2)*x1 \PYZhy{} b/w2}
\PY{n}{x1\PYZus{}vals} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}
\PY{k}{if} \PY{n}{perceptron}\PY{o}{.}\PY{n}{weights}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{!=} \PY{l+m+mi}{0}\PY{p}{:}
    \PY{n}{x2\PYZus{}vals} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{perceptron}\PY{o}{.}\PY{n}{weights}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{/} \PY{n}{perceptron}\PY{o}{.}\PY{n}{weights}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{o}{*} \PY{n}{x1\PYZus{}vals} \PY{o}{\PYZhy{}} \PY{n}{perceptron}\PY{o}{.}\PY{n}{bias} \PY{o}{/} \PY{n}{perceptron}\PY{o}{.}\PY{n}{weights}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
\PY{k}{else}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} If weights[1] is zero, the decision boundary is vertical.}
    \PY{n}{x1\PYZus{}vals} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{full}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{n}{perceptron}\PY{o}{.}\PY{n}{bias} \PY{o}{/} \PY{n}{perceptron}\PY{o}{.}\PY{n}{weights}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
    \PY{n}{x2\PYZus{}vals} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class 0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x1\PYZus{}vals}\PY{p}{,} \PY{n}{x2\PYZus{}vals}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Decision Boundary}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Perceptron Decision Boundary on a scikit\PYZhy{}learn Dataset}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Feature 1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Feature 2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Training Accuracy: 100.00\%
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nb-lesson-3-2_files/nb-lesson-3-2_15_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    At the end of the training the perceptron will make its best prediction,
having learned its weights and its bias. But the prediction could be
wrong. Can we understand why? Let's try to identify the problem simply
by looking at the graph. \textbf{How many different lines can we draw
that separate the two subsets of data?}

    \subsubsection{Matrix Notation}\label{matrix-notation}

To generalize the perceptron formulation using matrix notation, we first
express the linear combination in vectorized form and then show how this
extends to a collection of samples.

\textbf{Single Sample Formulation}

The original formulation for a single sample is:

$$
y = \begin{cases} 
1 & \text{if } \sum\limits_{i=1}^{n} w_i x_i + b \geq 0 \\
& \\
0 & \text{if } \sum\limits_{i=1}^{n} w_i x_i + b < 0
\end{cases}
$$

We can rewrite the sum using the dot product notation. Let:

$$\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \in \mathbb{R}^{n}$$

be the feature vector,

$$\mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix} \in \mathbb{R}^{n}$$

be the weight vector and $b$ be the bias (a scalar). Then, the linear
combination is written as:

$$
\mathbf{w}^\top \mathbf{x} + b.
$$

and the decision rule becomes:

$$
y = \begin{cases} 
1 & \text{if } \mathbf{w}^\top \mathbf{x} + b \geq 0 \\
& \\
0 & \text{if } \mathbf{w}^\top \mathbf{x} + b < 0
\end{cases}
$$

\textbf{Incorporating the Bias Term into the Vector Notation}

A common trick is to absorb the bias $b$ into the weight vector by
augmenting the input vector. Define:

$$\tilde{\mathbf{x}} = \begin{bmatrix} 1 \\ x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \in \mathbb{R}^{n+1}, \quad \text{(augmented input vector)}$$

$$\tilde{\mathbf{w}} = \begin{bmatrix} b \\ w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix} \in \mathbb{R}^{n+1}, \quad \text{ (augmented weight vector)}$$

Then the linear combination becomes:

$$
\tilde{\mathbf{w}}^\top \tilde{\mathbf{x}},
$$

and the decision rule is:

$$
y = \begin{cases} 
1 & \text{if } \tilde{\mathbf{w}}^\top \tilde{\mathbf{x}} \geq 0 \\
& \\
0 & \text{if } \tilde{\mathbf{w}}^\top \tilde{\mathbf{x}} < 0
\end{cases}
$$

\textbf{Multiple Samples}

If we have a dataset with $m$ samples, we can represent the entire
dataset as a matrix. Let:

\begin{itemize}
\tightlist
\item
  $X \in \mathbb{R}^{m \times n}$ be the matrix where each row
  corresponds to an input sample $\mathbf{x}^{(j)}$.
\item
  $\mathbf{w} \in \mathbb{R}^{n}$ be the weight vector.
\item
  $b$ be the bias.
\end{itemize}

The vectorized computation for all $m$ samples is:

$$
\mathbf{z} = X \mathbf{w} + b,
$$

where $\mathbf{z} \in \mathbb{R}^{m}$ is the vector of linear
combinations for each sample, and the addition of $b$ is typically
implemented with broadcasting (i.e., $b$ is added to each element of
$X \mathbf{w}$).

If you use the augmented representation, define:

\begin{itemize}
\tightlist
\item
  $\tilde{X} \in \mathbb{R}^{m \times (n+1)}$ where each row is the
  augmented input vector
  $\tilde{\mathbf{x}}^{(j)} = [1, x_1^{(j)}, x_2^{(j)}, \dots, x_n^{(j)}]$.
\item
  $\tilde{\mathbf{w}} \in \mathbb{R}^{n+1}$ be the augmented weight
  vector.
\end{itemize}

Then the computation for all samples becomes even simpler:

$$
\mathbf{z} = \tilde{X} \tilde{\mathbf{w}},
$$

and the decision rule applied element-wise is:

$$
y^{(j)} = \begin{cases} 
1 & \text{if } z^{(j)} \geq 0 \\
& \\
0 & \text{if } z^{(j)} < 0
\end{cases}
\quad \text{for } j = 1, 2, \ldots, m.
$$

\textbf{Summary}

\begin{itemize}
\item
  \textbf{Single Sample}:

  $$
  y = \begin{cases} 
  1 & \text{if } \mathbf{w}^\top \mathbf{x} + b \geq 0 \\
  & \\
  0 & \text{if } \mathbf{w}^\top \mathbf{x} + b < 0
  \end{cases}
  $$

  or equivalently using augmentation:

  $$
  y = \begin{cases} 
  1 & \text{if } \tilde{\mathbf{w}}^\top \tilde{\mathbf{x}} \geq 0 \\
  & \\
  0 & \text{if } \tilde{\mathbf{w}}^\top \tilde{\mathbf{x}} < 0
  \end{cases}
  $$
\item
  \textbf{Multiple Samples}:

  $$
  \mathbf{z} = X \mathbf{w} + b \quad \text{or} \quad \mathbf{z} = \tilde{X} \tilde{\mathbf{w}},
  $$

  and apply the step function element-wise to obtain the predictions for
  all samples.
\end{itemize}

This matrix notation not only simplifies the representation but also
makes it easier to implement and optimize the perceptron algorithm,
especially when dealing with large datasets.

    \subsubsection{The Problem of the
Perceptron}\label{the-problem-of-the-perceptron}

    The following picture provides an example of a perceptron algorithm that
identifies a hyperplane capable of separating circles from triangles.
This hyperplane is perfectly valid, as it is derived from the initial
dataset on which the perceptron is trained.

    

    Now, let us imagine receiving a new data point, specifically a triangle
located near the original group of triangles. The perceptron will
classify this point based on the hyperplane it identified during
training. Here is what it will do: it will classify the point as a
circle (depicted in gray), thereby making an error. It is easy to
understand why this happens. The decision boundaries line come so close
to the instances that the models will probably not perform as well on
new instances.

    

    It is easy to conceive of another hyperplane, represented by a dashed
gray line, rotated slightly with respect to the previous one. This new
hyperplane would correctly classify the new point as a triangle.

Naturally, even with this improved hyperplane, it is still possible for
another new point to be misclassified. Clearly, while one might envision
a third hyperplane that performs better, this method of manually
adjusting the hyperplane is not sustainable. Furthermore, let us not
forget that the process of identifying a hyperplane often (if not
always) involves working in dimensions far greater than two, making
visualization by the human mind impractical. A systematic method is
necessary to identify the best possible separating hyperplane that
minimizes classification errors for new data points.

    

    An SVM (Support Vector Machine) accomplishes precisely this: it
identifies an optimal hyperplane from an infinite number of possible
options.

The following figure illustrates Vapnik's algorithm. Given certain sets
of linearly separable points, the algorithm finds a hyperplane (depicted
as a black line) that maximizes the margin on both sides. Note that some
points are black, while others are gray. The black points are the
closest to the separating hyperplane. In the example, there is one black
circle and two black triangles. The separating hyperplane is equidistant
from the black circle and the two black triangles. It is as if we have
drawn a path through the ``forest'' of points---a ``no-man's land,'' so
to speak. By definition, no points lie within this path.

    

    The closest points from both clusters reach the maximum margins of this
path and are represented as black points. The hyperplane is the line
that passes through the middle of this path. Once an optimal hyperplane
is found, it is more likely to classify new points correctly as either a
circle or a triangle compared to the hyperplane identified by the
perceptron.

    \subsection{Support Vector Machines
(SVM)}\label{support-vector-machines-svm}

    SVM is an algorithm that takes the data as an input and outputs a line
that separates those classes if possible but in this case our
optimization objective is to maximize the margin.

The margin is defined as the distance between the separating hyperplane
(decision boundary) and the training examples that are closest to this
hyperplane, which are the so-called \textbf{support vectors}. This is
illustrated in the following figure:

    

    Thus SVM tries to make a decision boundary in such a way that the
separation between the two classes (the margin) is as wide as possible.

    \subsubsection{Hard and Soft Margins}\label{hard-and-soft-margins}

The rationale behind having decision boundaries with large margins is
that they tend to have a lower generalization error, whereas models with
small margins are more prone to overfitting. To get an idea of the
margin maximization, let's take a closer look at those positive and
negative hyperplanes that are parallel to the decision boundary.

When the data is linearly separable, and we don't want to have any
misclassifications, we use SVM with a \textbf{hard margin}. However,
when a linear boundary is not feasible, or we want to allow some
misclassifications in the hope of achieving better generality, we can
opt for a \textbf{soft margin} for our classifier.

    

    \subsubsection{The Algorithm}\label{the-algorithm}

\textbf{Step 1: Normalization of the SVM Hyperplanes}

In the SVM framework, the hyperplanes are defined as: - Upper margin:
$w_1 x_1 + w_2 x_2 = b_u $, - Lower margin: $w_1 x_1 + w_2
x_2 = b_d $, - Decision boundary (center): $ w_1 x_1 + w_2 x_2 +
b = 0 $.

    

    From the equation for the distance between two parallel lines (see
Appendix) we can write:

$$P = \frac{b_u - b_d}{\sqrt{w_1^2+w_2^2}}$$

We can scale $w_1$, $w_2$, $b_u$, and $b_d$ by the same constant
without changing the model. We can therefore choose a constant $alpha$
such that $\alpha b_u=b+1$ and $\alpha b_d=b-1$. With a bit of
algebra we easily find

$$
\alpha = \frac{2}{b_u-b_d}
$$

$$
b = \frac{b_d+b_u}{b_d-b_u}
$$

using this scaling we finally have

$$P = \frac{2}{\sqrt{w_1^2+w_2^2}} = \frac{2}{\vert\vert \mathbf{w} \vert\vert^2}$$

    In the \textbf{hard margin} case the algorithm minimizes $w_1^2+w_2^2$
subject to \textbf{perfect separation} being achieved

Now, the objective function of the SVM becomes the maximization of this
margin by maximizing $P$ under the constraint that the examples are
classified correctly, which can be written as:

$$w_0 + \mathbf{w}^T \cdot \mathbf{x}^{(i)} \ge 1 \quad \text{if } y^{(i)} = 1$$
$$w_0 + \mathbf{w}^T \cdot \mathbf{x}^{(i)} \le -1 \quad \text{if } y^{(i)} = -1$$

for $i = 1, \dots, N$. Here, $N$ is the number of examples in our dataset and $w_0 = -b$.

These two equations basically say that all negative-class examples
should fall on one side of the negative hyperplane, whereas all the
positive-class examples should fall behind the positive hyperplane,
which can also be written more compactly as follows:

\begin{equation}
y^{(i)}\left(w_0 + \mathbf{w}^T \cdot \mathbf{x}^{(i)}\right) \ge 1 \quad \forall i
\end{equation}

In practice, though, it is easier to minimize the reciprocal term,
$\frac{1}{2}\vert\vert\mathbf{w}\vert\vert^2$ , which can be solved by
\textbf{quadratic programming}.

    \subsubsection{Lagrange Multiplier}\label{lagrange-multiplier}

Because we have a contrained optimization problem (with inequality
contraints) we can recast the problem using Lagrange multipliers,
$\alpha \ge 0$ and search for the critical points of

\begin{equation}\label{eq:7.7}
L = \frac{1}{2} \vert\vert \mathbf{w} \vert\vert^2 - \sum\limits_{n=1}^N \alpha_n \left[ y^{(n)} \left( \mathbf{w}^T \mathbf{x}^{(n)} + w_0 \right) -1 \right]
\end{equation}

To find the critical points we differentiate with respect to the scalar
$w_0$ and with respect to the vector $\mathbf{w}$. Remember that
differentiation with respect to a vector just means differentiating with
respect to each of its entries. Setting these derivatives to zero you
end up with:

\begin{equation}\label{eq:7.8}
\frac{\partial L}{\partial w_0} = \sum\limits_{n=1}^N \alpha_n  y^{(n)} = 0
\end{equation}

and

\begin{equation}\label{eq:7.9}
\frac{\partial L}{\partial \mathbf{w}} = \mathbf{w} - \sum\limits_{n=1}^N \alpha_n  y^{(n)} \mathbf{x}^{(n)} = 0
\end{equation}

From the second equation we obtain immediatly a very important result:

$$
\mathbf{w} = \sum\limits_{n=1}^N \alpha_n  y^{(n)} \mathbf{x}^{(n)}
$$

which means that our vector orthogonal to the hyperplane is just a
linear combination of sample vectors. Substituting \eqref{eq:7.9} into
\eqref{eq:7.7} and using \eqref{eq:7.8} results in (see Appendix for all
the details):

\begin{equation}
L = \sum\limits_{n=1}^N \alpha_n - \frac{1}{2} \sum\limits_{i=1}^N \sum\limits_{j=1}^N \alpha_i \alpha_j y^{(i)}y^{(j)} 
{\mathbf{x}^{(i)}}^T \mathbf{x}^{(j)} 
\end{equation}

We want to maximize $L$ over the $\alpha$s, all greater than or
equal to zero, subject to \eqref{eq:7.8}.

    To find the coefficients $ \alpha_n $, we must solve the \textbf{dual
optimization problem} derived from the Lagrangian formulation.

\textbf{Step 1: Define the Optimization Problem}

The coefficients $ \alpha_n $ are found by \textbf{solving the dual
problem}:

$$
\max_{\alpha} L(\alpha)
$$

subject to the \textbf{constraints}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Non-negativity constraint}:\\
  $$
  \alpha_n \geq 0, \quad \forall n
  $$
\item
  \textbf{Equality constraint} (from the derivative with respect to $
  w_0 $):$$ \sum\limits_{n=1}^N \alpha_n y^n = 0
  $$
\end{enumerate}

This is a \textbf{quadratic programming (QP) problem}, which can be
solved using numerical optimization techniques.

\textbf{Step 2: Reformulate as a Quadratic Programming Problem}

Rewriting the optimization problem in a standard \textbf{QP form}:

$$
\max_{\boldsymbol{\alpha}} \quad \mathbf{1}^T \boldsymbol{\alpha} - \frac{1}{2} \boldsymbol{\alpha}^T Q \boldsymbol{\alpha}
$$

where:

\begin{itemize}
\item
  $ \boldsymbol{\alpha} = (\alpha_1, \alpha_2, \ldots,
  \alpha_N)\^{}T $ is the vector of Lagrange multipliers.
\item
  $ \mathbf{1} $ is a vector of ones.
\item
  $ Q $ is an \textbf{$N \times N$ matrix} with entries:

  $$
  Q_{ij} = y^{(i)} y^{(j)} \mathbf{x}^{(i)} \cdot \mathbf{x}^{(j)}
  $$
\end{itemize}

\textbf{Constraints in matrix form}:

$$
\text{subject to } \quad \mathbf{y}^T \boldsymbol{\alpha} = 0, \quad \boldsymbol{\alpha} \geq 0
$$

where $ \mathbf{y} = (y\^{}\{(1)\}, y\^{}\{(2)\}, \ldots,
y\textsuperscript{\{(N)\})}T $.

\textbf{Step 3: Solve Using Quadratic Programming}

This quadratic programming problem can be solved using numerical methods
such as:

\begin{itemize}
\tightlist
\item
  \textbf{Sequential Minimal Optimization (SMO)}: Used in \textbf{SVM
  solvers} like LIBSVM, Scikit-Learn's SVC, and CVXOPT.
\item
  \textbf{Interior Point Methods}: Standard QP solvers in optimization
  libraries.
\end{itemize}

\textbf{Step 4: Identify Support Vectors}

Once we obtain $ \alpha_n $, the \textbf{support vectors} correspond
to the nonzero values of $ \alpha_n $. The weight vector is then
computed as:

$$
\mathbf{w} = \sum\limits_{n=1}^N \alpha_n y^{(n)} \mathbf{x}^{(n)}
$$

To compute $ w_0 $, we use any support vector $
\mathbf{x}\^{}\{(s)\} $ such that $ \alpha_s \textgreater{} 0 $:

$$
w_0 = y^{(s)} - \mathbf{w}^T \mathbf{x}^{(s)}
$$

    \textbf{4. Classification of New Data} To classify a new point $
\mathbf{x}_\{\text{new}\} $, we compute:

$$
f(\mathbf{x}_{\text{new}}) = \mathbf{w}^T \mathbf{x}_{\text{new}} + w_0
$$

If $ f(\mathbf{x}_\{\text{new}\}) \textgreater{} 0 $, classify as $
+1 $, otherwise classify as $ -1 $.

\textbf{Summary}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Solve the QP problem} to get $ \alpha_n $.
\item
  \textbf{Compute $ \mathbf{w} $ as a linear combination of support
  vectors}.
\item
  \textbf{Find $ w_0 $ using a support vector}.
\item
  \textbf{Use $ f(\mathbf{x}) = \mathbf{w}\^{}T \mathbf{x} + w_0 $
  for classification}.
\end{enumerate}

\textbf{Key Takeaways}

\begin{itemize}
\tightlist
\item
  $ \alpha_n $ is found by solving a \textbf{Quadratic Programming
  problem}.
\item
  \textbf{Support vectors} are those with nonzero $ \alpha_n $.
\item
  \textbf{Decision boundary} is computed using $ \mathbf{w} $ and $
  w_0 $.
\item
  \textbf{New data points} are classified using the sign of $
  f(\mathbf{x}) $.
\end{itemize}

    \subsubsection{Boundary Optimization with Soft
Margins}\label{boundary-optimization-with-soft-margins}

Let's briefly mention the slack variable, $\xi$ , which was introduced
by Vladimir Vapnik in 1995 and led to the so-called soft-margin
classification. The motivation for introducing the slack variable,
$\xi$ , was that the linear constraints need to be relaxed for
nonlinearly separable data to allow the convergence of the optimization
in the presence of misclassifications, under appropriate cost
penalization.

The positive-valued slack variable is simply added to the linear
constraints:

$$b + \mathbf{w}^𝑇 \cdot \mathbf{x}^{(𝑖)} \ge 1  - \xi^{(i)}\quad \text{if } 𝑦^{(𝑖)} = 1$$

$$b + \mathbf{w}^𝑇 \cdot \mathbf{x}^{(𝑖)} \le -1  + \xi^{(i)}\quad \text{if } 𝑦^{(𝑖)} = -1$$

So, the new objective to be minimized (subject to the contraints)
becomes

\begin{equation}
\frac{1}{2} \vert\vert \mathbf{w} \vert\vert^2 + C \left( \sum_i \xi^{(i)} \right)
\end{equation}

Via the variable, C, we can then control the penalty for
misclassification. Large values of C correspond to large error
penalties, whereas we are less strict about misclassification errors if
we choose smaller values for C. We can then use the C parameter to
control the width of the margin and therefore tune the bias-variance
tradeoff.

    \subsubsection{SVC Method in scikit-learn
Library}\label{svc-method-in-scikit-learn-library}

The \textbf{SVC (Support Vector Classification)} method in the
\textbf{scikit-learn} library is a powerful tool for solving
classification problems using the \textbf{Support Vector Machine (SVM)}
framework. It is implemented in the \texttt{sklearn.svm} module and
allows for both linear and non-linear classification using different
kernels.

\paragraph{\texorpdfstring{\textbf{How SVC
Works}}{How SVC Works}}\label{how-svc-works}

SVC attempts to find the optimal \textbf{decision boundary} that
separates different classes in the feature space. In the \textbf{linear
case}, it finds a hyperplane defined by the weight vector $ \mathbf{w}
$ and bias term $ w_0 $. If the classes are not \textbf{linearly
separable}, it uses a \textbf{soft margin} approach, allowing some
misclassification controlled by a regularization parameter $ C $. In
\textbf{non-linear classification}, it employs \textbf{kernel functions}
to transform the data into a higher-dimensional space where a linear
separation is possible.

\paragraph{\texorpdfstring{\textbf{Key
Parameters}}{Key Parameters}}\label{key-parameters}

\begin{itemize}
\tightlist
\item
  \texttt{C}: The \textbf{regularization parameter} that controls the
  trade-off between achieving a low error and maximizing the margin. A
  \textbf{small $ C $} allows a wider margin but may lead to more
  misclassifications, while a \textbf{large $ C $} aims for perfect
  classification but may reduce generalization.
\item
  \texttt{kernel}: Determines the type of transformation applied to the
  data. Options include:

  \begin{itemize}
  \tightlist
  \item
    \texttt{"linear"}: Uses a simple dot product $ \mathbf{x}_i\^{}T
    \mathbf{x}_j $ for linearly separable data.
  \item
    \texttt{"poly"}: Uses a \textbf{polynomial kernel} $ (
    \mathbf{x}_i\^{}T \mathbf{x}_j + r )\^{}d $ to capture non-linear
    patterns.
  \item
    \texttt{"rbf"}: The \textbf{Radial Basis Function (RBF) kernel} $
    e\^{}\{-\gamma \textbar{}\mathbf{x}_i -
    \mathbf{x}_j\textbar\^{}2\} $ introduces flexibility and is
    commonly used.
  \item
    \texttt{"sigmoid"}: Uses the \textbf{sigmoid kernel}, similar to
    neural networks.
  \end{itemize}
\item
  \texttt{gamma}: Controls the influence of individual training samples
  in \textbf{RBF} and \textbf{polynomial kernels}. A \textbf{high $
  \gamma $} leads to a more flexible model that fits the training data
  closely, while a \textbf{low $ \gamma $} results in a smoother
  decision boundary.
\item
  \texttt{degree}: Specifies the degree of the polynomial when using the
  \texttt{"poly"} kernel.
\item
  \texttt{probability}: Enables \textbf{probabilistic predictions} by
  computing \textbf{class probabilities} using Platt scaling, though it
  increases computation time.
\item
  \texttt{shrinking}: Whether to use the \textbf{Shrinking heuristic},
  which speeds up training by eliminating non-support vectors early.
\end{itemize}

\paragraph{\texorpdfstring{\textbf{Fitting a
Model}}{Fitting a Model}}\label{fitting-a-model}

To train an \textbf{SVC model}, we first import the necessary module and
create an instance of \texttt{SVC}. We then use the \texttt{.fit()}
method to train the classifier on the dataset.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k+kn}{import} \PY{n}{SVC}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}classification}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}

\PY{c+c1}{\PYZsh{} Generate synthetic data}
\PY{n}{custom\PYZus{}centers} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{]}
\PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{make\PYZus{}blobs}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{centers}\PY{o}{=}\PY{n}{custom\PYZus{}centers}\PY{p}{,} \PY{n}{n\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{cluster\PYZus{}std}\PY{o}{=}\PY{l+m+mf}{1.6}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Create the scatter plot}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Class 0}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{o}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.7}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Class 1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{s}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.7}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Set labels, title, and legend}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Feature 1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Feature 2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Scatter Plot of Generated Classification Data}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Display the plot}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nb-lesson-3-2_files/nb-lesson-3-2_43_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Split into training and testing sets}
\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Create and train an SVC model}
\PY{n}{model} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{linear}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{)}
\PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
SVC(kernel='linear')
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{\texorpdfstring{\textbf{Making
Predictions}}{Making Predictions}}\label{making-predictions}

Once trained, we use the \texttt{.predict()} method to classify new
samples.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \paragraph{\texorpdfstring{\textbf{Extracting Support
Vectors}}{Extracting Support Vectors}}\label{extracting-support-vectors}

The trained model stores the \textbf{support vectors}, which define the
decision boundary. These can be accessed via:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{support_vectors }\OperatorTok{=}\NormalTok{ model.support_vectors_}
\end{Highlighting}
\end{Shaded}

The \textbf{indices of the support vectors} in the training set are
available as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{support_indices }\OperatorTok{=}\NormalTok{ model.support_}
\end{Highlighting}
\end{Shaded}

The \textbf{Lagrange multipliers (alphas)} associated with support
vectors can be retrieved using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dual_coefs }\OperatorTok{=}\NormalTok{ model.dual_coef_}
\end{Highlighting}
\end{Shaded}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{w} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{b} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{model}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{w}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{b}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[0.24368569 0.38611903]
-0.048565855213413854
    \end{Verbatim}

    Remember that the outer lines are $w_1x_1+w_2x_2 = b_u$ and
$w_1x_1+w_2x_2=b_d$. The middle line is $w_1x_1+w_2x_2=b$, where
$b_u=b+1$ and $b_d=b-1$. The width of the path is

$$\frac{2}{\sqrt{w_1^2+w_2^2}}$$.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}

\PY{c+c1}{\PYZsh{} Generate a range of x\PYZhy{}values for plotting the decision boundary and margins}
\PY{n}{x1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Create 250 points evenly spaced between \PYZhy{}10 and 10}

\PY{c+c1}{\PYZsh{} Extract weight components from the SVM model (assuming w is a trained weight vector)}
\PY{n}{w1} \PY{o}{=} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}  \PY{c+c1}{\PYZsh{} Weight for the first feature}
\PY{n}{w2} \PY{o}{=} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}  \PY{c+c1}{\PYZsh{} Weight for the second feature}

\PY{c+c1}{\PYZsh{} Compute the upper and lower margin boundaries}
\PY{n}{bu} \PY{o}{=} \PY{n}{b} \PY{o}{+} \PY{l+m+mi}{1}  \PY{c+c1}{\PYZsh{} Upper margin (b is the SVM bias term)}
\PY{n}{bd} \PY{o}{=} \PY{n}{b} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}  \PY{c+c1}{\PYZsh{} Lower margin}

\PY{c+c1}{\PYZsh{} Compute the decision boundary and margins using the SVM equation: w1*x1 + w2*x2 + b = 0}
\PY{c+c1}{\PYZsh{} Rearranging for x2 (y\PYZhy{}axis): x2 = (\PYZhy{}b \PYZhy{} w1*x1) / w2}
\PY{n}{y1} \PY{o}{=} \PY{p}{(}\PY{n}{bu} \PY{o}{\PYZhy{}} \PY{n}{w1} \PY{o}{*} \PY{n}{x1}\PY{p}{)} \PY{o}{/} \PY{n}{w2}  \PY{c+c1}{\PYZsh{} Upper margin}
\PY{n}{y2} \PY{o}{=} \PY{p}{(}\PY{n}{bd} \PY{o}{\PYZhy{}} \PY{n}{w1} \PY{o}{*} \PY{n}{x1}\PY{p}{)} \PY{o}{/} \PY{n}{w2}  \PY{c+c1}{\PYZsh{} Lower margin}
\PY{n}{y0} \PY{o}{=} \PY{p}{(}\PY{n}{b}  \PY{o}{\PYZhy{}} \PY{n}{w1} \PY{o}{*} \PY{n}{x1}\PY{p}{)} \PY{o}{/} \PY{n}{w2}  \PY{c+c1}{\PYZsh{} Decision boundary}

\PY{c+c1}{\PYZsh{} Create the plot}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Set figure size for better visualization}

\PY{c+c1}{\PYZsh{} Scatter plot of the dataset, with different colors for each class}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Class 0}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{o}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.7}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Class 1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{s}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.7}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot the decision boundary and margins}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x1}\PY{p}{,} \PY{n}{y1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Upper Margin}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Upper margin (dashed green)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x1}\PY{p}{,} \PY{n}{y2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lower Margin}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Lower margin (dashed green)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x1}\PY{p}{,} \PY{n}{y0}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Decision Boundary}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Decision boundary (solid blue)}

\PY{c+c1}{\PYZsh{} Label the axes}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Label for x\PYZhy{}axis (Feature 1)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Label for y\PYZhy{}axis (Feature 2)}

\PY{c+c1}{\PYZsh{} Show legend to differentiate decision boundary and margins}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Display the plot}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nb-lesson-3-2_files/nb-lesson-3-2_50_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{\texorpdfstring{\textbf{Evaluating
Performance}}{Evaluating Performance}}\label{evaluating-performance}

To assess the classifier's accuracy, we compare predictions with actual
labels using standard evaluation metrics.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{accuracy\PYZus{}score}\PY{p}{,} \PY{n}{classification\PYZus{}report}

\PY{c+c1}{\PYZsh{} Accuracy}
\PY{n}{accuracy} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Detailed classification report}
\PY{n}{report} \PY{o}{=} \PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{report}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy: 1.0
              precision    recall  f1-score   support

           0       1.00      1.00      1.00       107
           1       1.00      1.00      1.00        93

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

    \end{Verbatim}

    \paragraph{\texorpdfstring{\textbf{Hyperparameter
Tuning}}{Hyperparameter Tuning}}\label{hyperparameter-tuning}

To find the best hyperparameters for \textbf{SVC}, we use
\texttt{GridSearchCV} for an exhaustive search or
\texttt{RandomizedSearchCV} for a randomized search.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{GridSearchCV}

\PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{kernel}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{linear}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rbf}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{poly}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{gamma}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{scale}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{auto}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{p}{\PYZcb{}}

\PY{n}{grid\PYZus{}search} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{SVC}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Best parameters}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best parameters:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Best parameters: \{'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'\}
    \end{Verbatim}

    \paragraph{\texorpdfstring{\textbf{Using SVC for
Inference}}{Using SVC for Inference}}\label{using-svc-for-inference}

For \textbf{real-time classification}, we define a function that takes a
new data point and predicts the class.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{classify\PYZus{}new}\PY{p}{(}\PY{n}{sample}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{p}{[}\PY{n}{sample}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Example classification}
\PY{n}{new\PYZus{}point} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.2}\PY{p}{]}
\PY{n}{prediction} \PY{o}{=} \PY{n}{classify\PYZus{}new}\PY{p}{(}\PY{n}{new\PYZus{}point}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Predicted class:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{prediction}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Predicted class: 0
    \end{Verbatim}

    \subsection{Kernel Methods in Support Vector
Machines}\label{kernel-methods-in-support-vector-machines}

\subsubsection{Non Linear Separable
Dataset}\label{non-linear-separable-dataset}

As we have seen, the effectiveness of SVMs largely depends on their
ability to find an optimal decision boundary, which is typically a
hyperplane in a high-dimensional feature space. However, many real-world
problems involve data that is not linearly separable in its original
space. As we are going to study in this section, to address this
limitation, we can transform the problem into a higher-dimensional space
where a linear separation becomes possible.~ This transformation is
achieved through a mapping function $\phi(x)$ that projects the data
into a new feature space.

The challenge with this approach is that explicitly computing the
transformation can be computationally expensive or even infeasible.
Kernel methods circumvent this issue by implicitly performing the
transformation through a kernel function, allowing SVMs to effectively
handle non-linearly separable data without explicitly computing the
mapped feature vectors.

But let's start with a practical example. In the figure below, there is
no way to draw a straight line or a linear separation hyperplane to
distinguish the circles from the triangles.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}circles}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k+kn}{import} \PY{n}{SVC}
\PY{k+kn}{from} \PY{n+nn}{mpl\PYZus{}toolkits}\PY{n+nn}{.}\PY{n+nn}{mplot3d} \PY{k+kn}{import} \PY{n}{Axes3D}

\PY{c+c1}{\PYZsh{} Generazione di dati casuali a simmetria circolare}
\PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{make\PYZus{}circles}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{factor}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{noise}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Visualizzazione dei dati originali in 2D}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Classe 0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Classe 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dati originali in 2D}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nb-lesson-3-2_files/nb-lesson-3-2_58_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{The Kernel Trick}\label{the-kernel-trick}

If we project these data points into three dimensions, so that the
triangles are elevated above the circles, we can find a separation
hyperplane. Here is one way to do it. Each original point is
two-dimensional, characterized by the values of $x_1$ and $x_2$
along the two axes and by a label $y$, which can be 1 (circle) or -1
(triangle). We can project these data points into three dimensions by
creating a third feature, $x_1^2 + x_2^2$, which can be plotted on the
$z$-axis. Thus, now each point in three dimensions is represented by
$x_1$, $x_2$, and $x_1^2 + x_2^2$, which are values along the
$x$, $y$, and $z$ axes, respectively. When plotted in 3D, the
triangles rise above the circles, and a hyperplane can be found (as
shown in the figure) that separates the two classes.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k+kn}{import} \PY{n}{SVC}

\PY{c+c1}{\PYZsh{} Add the possibility to modify the rotation of the plots}
\PY{k}{def} \PY{n+nf}{plot\PYZus{}3d\PYZus{}with\PYZus{}rotation}\PY{p}{(}\PY{n}{elev1}\PY{p}{,} \PY{n}{azim1}\PY{p}{,} \PY{n}{elev2}\PY{p}{,} \PY{n}{azim2}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Generate synthetic data for the demonstration (example)}
    \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{42}\PY{p}{)}
    \PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{make\PYZus{}circles}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{factor}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{noise}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Add a non\PYZhy{}linear feature (map to a higher dimension) to make the data separable using a linear kernel}
    \PY{n}{X1} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}  \PY{c+c1}{\PYZsh{} First feature}
    \PY{n}{X2} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}  \PY{c+c1}{\PYZsh{} Second feature}
    \PY{n}{X3} \PY{o}{=} \PY{n}{X1} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2} \PY{o}{+} \PY{n}{X2} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}  \PY{c+c1}{\PYZsh{} Non\PYZhy{}linear mapping to a higher dimension}

    \PY{c+c1}{\PYZsh{} Create the figure}
    \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} First subplot: Scatter plot of the data in 3D (with the non\PYZhy{}linear feature added)}
    \PY{n}{ax1} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{121}\PY{p}{,} \PY{n}{projection}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{3d}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X1}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X2}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X3}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class 0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Points for Class 0}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X1}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{X2}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{X3}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Points for Class 1}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Data in 3D (Non\PYZhy{}linear Mapping)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}zlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{view\PYZus{}init}\PY{p}{(}\PY{n}{elev}\PY{o}{=}\PY{n}{elev1}\PY{p}{,} \PY{n}{azim}\PY{o}{=}\PY{n}{azim1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Set the rotation for the first plot}

    \PY{c+c1}{\PYZsh{} Train an SVM with a linear kernel using the transformed 3D data}
    \PY{n}{clf} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Support Vector Classifier with linear kernel}
    \PY{n}{X\PYZus{}3d} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{c\PYZus{}}\PY{p}{[}\PY{n}{X1}\PY{p}{,} \PY{n}{X2}\PY{p}{,} \PY{n}{X3}\PY{p}{]}  \PY{c+c1}{\PYZsh{} Combine the 3 features into a single dataset}
    \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}3d}\PY{p}{,} \PY{n}{y}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Train the model on the 3D data}

    \PY{c+c1}{\PYZsh{} Second subplot: Visualize the separating hyperplane in 3D}
    \PY{n}{ax2} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{122}\PY{p}{,} \PY{n}{projection}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{3d}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X1}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X2}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X3}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class 0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Points for Class 0}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X1}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{X2}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{X3}\PY{p}{[}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Points for Class 1}

    \PY{c+c1}{\PYZsh{} Extract the hyperplane parameters (weights and bias)}
    \PY{n}{w} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}  \PY{c+c1}{\PYZsh{} Coefficients for the hyperplane}
    \PY{n}{b} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}  \PY{c+c1}{\PYZsh{} Bias term}

    \PY{c+c1}{\PYZsh{} Generate a grid to plot the hyperplane}
    \PY{n}{x1\PYZus{}range} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{X1}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X1}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Range for X1}
    \PY{n}{x2\PYZus{}range} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{X2}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X2}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Range for X2}
    \PY{n}{X1\PYZus{}grid}\PY{p}{,} \PY{n}{X2\PYZus{}grid} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{x1\PYZus{}range}\PY{p}{,} \PY{n}{x2\PYZus{}range}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Create a grid of X1 and X2 values}
    \PY{n}{X3\PYZus{}grid} \PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{w}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{n}{X1\PYZus{}grid} \PY{o}{\PYZhy{}} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{X2\PYZus{}grid} \PY{o}{\PYZhy{}} \PY{n}{b}\PY{p}{)} \PY{o}{/} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}  \PY{c+c1}{\PYZsh{} Compute X3 for the hyperplane}

    \PY{c+c1}{\PYZsh{} Plot the hyperplane as a surface}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{plot\PYZus{}surface}\PY{p}{(}\PY{n}{X1\PYZus{}grid}\PY{p}{,} \PY{n}{X2\PYZus{}grid}\PY{p}{,} \PY{n}{X3\PYZus{}grid}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yellow}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Semi\PYZhy{}transparent yellow surface}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Linear Separating Hyperplane in 3D}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{set\PYZus{}zlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{view\PYZus{}init}\PY{p}{(}\PY{n}{elev}\PY{o}{=}\PY{n}{elev2}\PY{p}{,} \PY{n}{azim}\PY{o}{=}\PY{n}{azim2}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Set the rotation for the second plot}

    \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Adjust layout to prevent overlapping elements}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Call the function with specific rotation angles for the two plots}
\PY{n}{plot\PYZus{}3d\PYZus{}with\PYZus{}rotation}\PY{p}{(}\PY{n}{elev1}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{azim1}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{,} \PY{n}{elev2}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{azim2}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nb-lesson-3-2_files/nb-lesson-3-2_61_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    For the problem we just tackled, it was not difficult to find a third
feature that allowed us to separate the data into two groups. But what
happens if the 2D data looks like the figure below?

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{34}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}

\PY{n}{X\PYZus{}xor} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{200}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{y\PYZus{}xor} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logical\PYZus{}xor}\PY{p}{(}\PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{y\PYZus{}xor} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{y\PYZus{}xor}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{n}{y\PYZus{}xor} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{n}{y\PYZus{}xor} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{n}{y\PYZus{}xor} \PY{o}{==} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{n}{y\PYZus{}xor} \PY{o}{==} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nb-lesson-3-2_files/nb-lesson-3-2_63_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Now it's not immediately clear what to choose as the third feature;
$x_1^2 + x_2^2$ no longer works. We need a rigorous method to project
the data into higher dimensions. It must work even if the
low-dimensional space is itself much higher than 2D (and thus impossible
to visualize). Moreover, once the data is projected into higher
dimensions, finding a linearly separable hyperplane in the augmented
space requires computing the dot product of vectors in higher
dimensions, which, as we have said, can become computationally
unmanageable.

So, in some way, the algorithm must simultaneously achieve two things:\\
- 1. \textbf{create new features} so that they can be mapped into a
higher-dimensional space, and

\begin{itemize}
\tightlist
\item
  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{1}
  \tightlist
  \item
    \textbf{avoid performing dot products} in this new space while still
    being able to find the separating hyperplane.
  \end{enumerate}
\end{itemize}

Let's start with data in two dimensions and map it into three dimensions
using three features.\\
Given a vector $x$ in the lower-dimensional space (2D in our case), it
is mapped to the vector $\phi(x)$ in the higher-dimensional space (3D
in our case):

$$
x \to \phi(x)
$$

Our mapping is as follows:
$[x_1 \; x_2] \to [x_1^2 \; x_2^2 \; \sqrt{2}x_1x_2]$.\\
Thus, if a point $a$ in 2D is given by $[a_1 \; a_2]$ and a point
$b$ is given by $[b_1 \; b_2]$, the same points, projected into 3D
space, become:

$$
[a_1^2 \; a_2^2 \; \sqrt{2}a_1a_2], \quad [b_1^2 \; b_2^2 \; \sqrt{2}b_1b_2].
$$

To find a linearly separable hyperplane, we would need to compute the
dot products of the points' vectors in the higher-dimensional space. In
this example, it is not a problem to compute the dot products of all
vectors in 3D space.\\
Unfortunately, in the real world, the dimensionality of the augmented
space can be enormous, making such calculations computationally
prohibitive in terms of resources (both time and memory). However,
Aizerman, Braverman, and Rozonoer {[}REF{]} devised a very useful trick
that completely bypassed this difficulty.

    In other words, to find the linearly separable hyperplane in the
higher-dimensional space, we would need to compute the dot products of
$\phi(x_i)$ with $\phi(x_j)$, for all combinations of $i$ and
$j$. But what if we could perform these calculations with the two
lower-dimensional vectors, $x_i$ and $x_j$, which give the same
result as the dot product of the vectors corresponding to them in the
higher-dimensional space? What if we could find a function $K$ such
that:

$$
K(x_i, x_j) \to \phi(x_i) \cdot \phi(x_j)
$$

In other words, if the two lower-dimensional vectors are passed to the
function $K$, it should return a value equal to the dot product of the
vectors projected into the higher-dimensional space. Let's look at a
concrete example of the vectors $a$ and $b$:

$$
a = [a_1 \; a_2], \quad b = [b_1 \; b_2].
$$

\begin{align*}
\phi(a) &= [a_1^2, a_2^2, \sqrt{2}a_1a_2] \\
& \\
\phi(b) &= [b_1^2, b_2^2, \sqrt{2}b_1b_2]
\end{align*}

\begin{align*}
\phi(a) \cdot \phi(b) &= [a_1^2, a_2^2, \sqrt{2}a_1a_2] \cdot [b_1^2, b_2^2, \sqrt{2}b_1b_2]\\
& \\
& = (a_1^2b_1^2 + a_2^2b_2^2 + 2a_1a_2b_1b_2)
\end{align*}

We need a function $K$ that produces the same result. Here is one such
function:

$$
K(x, y) = (x \cdot y)^2
$$

Let's feed this function with the two lower-dimensional vectors $a$
and $b$ and see what happens:

\begin{align*}
K(a, b) &= (a \cdot b)^2 = ([a_1, a_2] \cdot [b_1, b_2])^2 \\
&= (a_1b_1 + a_2b_2)^2 \\
&= (a_1^2b_1^2 + a_2^2b_2^2 + 2a_1a_2b_1b_2)
\end{align*}

So:

$$
K(a, b) = \phi(a) \cdot \phi(b)
$$

Since we have only worked with 2D spaces, the significance of this fact
may not be immediately obvious. Imagine, for a moment, that $a$ and
$b$ are 100-dimensional vectors and that $\phi(a)$ and $\phi(b)$
have a million dimensions. If we can find an appropriate mapping
$\phi(x)$ such that $K(x, x) = \phi(x) \cdot \phi(x)$, then we can
compute the dot products of high-dimensional vectors without ever
explicitly working in that million-dimensional space. We can perform the
calculations in ``just'' 100 dimensions.

The function $K$ is called a ``kernel function.'' The method that uses
the kernel function to compute dot products in a higher-dimensional
space without explicitly transforming each lower-dimensional vector into
its monstrously large counterpart is sometimes called the ``kernel
trick.'' It is indeed a very useful trick.

\subsubsection{The Polynomial Kernel}\label{the-polynomial-kernel}

SVC Method can use a number of kernel functions. One of them is known as
the ``polynomial kernel,'' introduced by MIT computational
neuroscientist Tomaso Poggio in 1975. The general form of the polynomial
kernel is:

$$
K(x, y) = (c + x \cdot y)^d, \quad \text{where } c \text{ and } d \text{ are two constants.}
$$

In the specific case where the constants are $c = 0$ and $d = 2$, we
obtain the kernel we just used for the previous example:

$$
K(x, y) = (x \cdot y)^2
$$

Let's experiment with another variant, where the constants are $c = 1$
and $d = 2$, to better understand why it works.

$$
K(x, y) = (1 + x \cdot y)^2
$$

For 2D points:

$$
a = [a_1 \; a_2], \quad b = [b_1 \; b_2]
$$

We get:

\begin{align*}
K(a, b) &= (1 + [a_1 \; a_2] \cdot [b_1 \; b_2])^2 \\
&= (1 + a_1b_1 + a_2b_2)^2 \\
&= 1 + (a_1b_1 + a_2b_2) + 2(a_1b_1 + a_2b_2) \\
&= 1 + a_1^2b_1^2 + a_2^2b_2^2 + 2a_1a_2b_1b_2 + 2a_1b_1 + 2a_2b_2
\end{align*}

The question now is: what must the mapping $\phi(x)$ be such that:

$$
K(x, x) = \phi(x) \cdot \phi(x)
$$

With a bit of patience, we can discover this mapping:

$$
x \to \phi(x)
$$

$$
\Rightarrow [x_1, x_2] \to [1, x_1^2, x_2^2, \sqrt{2}x_1x_2, \sqrt{2}x_1, \sqrt{2}x_2]
$$

I have added commas to separate the elements of the larger vector just
for clarity; normally, they are not used.

So:

$$
a = [a_1, a_2] \to [1, a_1^2, a_2^2, \sqrt{2}a_1a_2, \sqrt{2}a_1, \sqrt{2}a_2]
$$

$$
b = [b_1, b_2] \to [1, b_1^2, b_2^2, \sqrt{2}b_1b_2, \sqrt{2}b_1, \sqrt{2}b_2]
$$

We now have the 2D coordinates, or vectors, $a$ and $b$ transformed
into their 6D counterparts, $\phi(a)$ and $\phi(b)$. The question
is: does $\phi(a) \cdot \phi(b)$, evaluated in 6D space, yield the
same result as the kernel function applied to $a$ and $b$ in 2D
space? Let's verify:

\begin{align*}
\phi(a) \cdot \phi(b) & = [1, a_1^2, a_2^2, \sqrt{2}a_1a_2, \sqrt{2}a_1, \sqrt{2}a_2] \cdot [1, b_1^2, b_2^2, \sqrt{2}b_1b_2, \sqrt{2}b_1, \sqrt{2}b_2] \\
& = 1 + a_1^2b_1^2 + a_2^2b_2^2 + 2a_1a_2b_1b_2 + 2a_1b_1 + 2a_2b_2 \\
& = K(a, b)
\end{align*}

They are equal. Therefore, the kernel function allows us to compute the
dot product of 6D vectors without explicitly computing them in 6D space.
For our polynomial kernel, we used the values 1 and 2 for the two
constants, respectively.

It can be shown that the kernel function works for any values of these
constants. Thus, we can project data into increasingly higher
dimensions, where it is more likely to find a linearly separable
hyperplane.

As a side note, the dimensionality of the higher-dimensional space is:

$$
\frac{(n + d)!}{n! \, d!}
$$

where $n$ is the dimensionality of the original lower-dimensional
space, and $d$ is the value of the constant used in the polynomial
kernel.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k+kn}{import} \PY{n}{SVC}

\PY{c+c1}{\PYZsh{} Generate XOR dataset}
\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{X\PYZus{}xor} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{200}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{y\PYZus{}xor} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logical\PYZus{}xor}\PY{p}{(}\PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{y\PYZus{}xor} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{y\PYZus{}xor}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Train SVM with polynomial kernel}
\PY{n}{svm\PYZus{}poly} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{poly}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{degree}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{coef0}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}  \PY{c+c1}{\PYZsh{} degree=2 works well for XOR}
\PY{n}{svm\PYZus{}poly}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}xor}\PY{p}{,} \PY{n}{y\PYZus{}xor}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot decision boundary}
\PY{n}{xx}\PY{p}{,} \PY{n}{yy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\PY{n}{Z} \PY{o}{=} \PY{n}{svm\PYZus{}poly}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{c\PYZus{}}\PY{p}{[}\PY{n}{xx}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{yy}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\PY{n}{Z} \PY{o}{=} \PY{n}{Z}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{shape}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{contourf}\PY{p}{(}\PY{n}{xx}\PY{p}{,} \PY{n}{yy}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{n}{y\PYZus{}xor} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{n}{y\PYZus{}xor} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{n}{y\PYZus{}xor} \PY{o}{==} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{n}{y\PYZus{}xor} \PY{o}{==} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SVM with Polynomial Kernel (Degree=2) on XOR Data}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nb-lesson-3-2_files/nb-lesson-3-2_66_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Understanding the Radial Basis Function (RBF) Kernel in
SVM}\label{understanding-the-radial-basis-function-rbf-kernel-in-svm}

The \textbf{Radial Basis Function (RBF) kernel} is another powerful tool
that allows SVM to handle complex, nonlinear decision boundaries.
Instead of working directly with the original data, the RBF kernel
transforms it into a new space where it becomes easier to separate
classes.

The fundamental idea behind the RBF kernel is that data points that are
\textbf{close to each other} should have a \textbf{stronger
relationship} than points that are far apart. The function measures the
similarity between two points based on their \textbf{Euclidean
distance}. If two points are very close, the function assigns them a
high similarity value; if they are far apart, the similarity decreases
exponentially.

Mathematically, the RBF kernel is defined as:

$$
K(x, x') = \exp\left(-\gamma \| x - x' \|^2\right)
$$

where $ x $ and $ x' $ are two data points, $ \textbar{} x - x'
\textbar\^{}2 $ represents their squared Euclidean distance, and $
\gamma $ is a parameter that controls how much influence a single
training example has.

The parameter $ \gamma $ plays a crucial role in determining the
behavior of the RBF kernel. A \textbf{small $ \gamma $} means that the
function varies smoothly, meaning that even distant points are
considered similar to some extent. A \textbf{large $ \gamma $}, on the
other hand, makes the function decay rapidly, meaning that only points
very close to each other are considered similar. Choosing an appropriate
$ \gamma $ is essential for ensuring good model performance.

One of the key advantages of the RBF kernel is that it allows SVM to
create flexible decision boundaries that can adapt to complex patterns
in the data. Unlike polynomial kernels, which can lead to overly complex
models with high-degree polynomials, the RBF kernel provides a more
balanced approach by smoothly adapting to the structure of the data.

To see how the RBF kernel works in practice, consider an example where
we classify two groups of points that are not linearly separable. In the
original space, no straight line can effectively separate them. However,
when we apply the RBF kernel, the data is projected into a
higher-dimensional space, making it possible for SVM to find a linear
separation.

\paragraph{Example Using Python and
Scikit-Learn}\label{example-using-python-and-scikit-learn}

Let's implement an example using \texttt{scikit-learn} to demonstrate
how an SVM with an RBF kernel can classify non-linearly separable data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k+kn}{import} \PY{n}{SVC}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}moons}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{accuracy\PYZus{}score}

\PY{c+c1}{\PYZsh{} Generate synthetic data (moons dataset)}
\PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{make\PYZus{}moons}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{l+m+mi}{300}\PY{p}{,} \PY{n}{noise}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Split data into training and testing sets}
\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Train an SVM with RBF kernel}
\PY{n}{svm\PYZus{}rbf} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{svm\PYZus{}rbf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Predict on the test set}
\PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{svm\PYZus{}rbf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Compute accuracy}
\PY{n}{accuracy} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy of SVM with RBF kernel: }\PY{l+s+si}{\PYZob{}}\PY{n}{accuracy}\PY{l+s+si}{:}\PY{l+s+s2}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot decision boundary}
\PY{k}{def} \PY{n+nf}{plot\PYZus{}decision\PYZus{}boundary}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
    \PY{n}{h} \PY{o}{=} \PY{l+m+mf}{0.02}
    \PY{n}{x\PYZus{}min}\PY{p}{,} \PY{n}{x\PYZus{}max} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}
    \PY{n}{y\PYZus{}min}\PY{p}{,} \PY{n}{y\PYZus{}max} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}
    \PY{n}{xx}\PY{p}{,} \PY{n}{yy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{x\PYZus{}min}\PY{p}{,} \PY{n}{x\PYZus{}max}\PY{p}{,} \PY{n}{h}\PY{p}{)}\PY{p}{,}
                         \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{y\PYZus{}min}\PY{p}{,} \PY{n}{y\PYZus{}max}\PY{p}{,} \PY{n}{h}\PY{p}{)}\PY{p}{)}
    \PY{n}{Z} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{c\PYZus{}}\PY{p}{[}\PY{n}{xx}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{yy}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{)}
    \PY{n}{Z} \PY{o}{=} \PY{n}{Z}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
    
    \PY{n}{plt}\PY{o}{.}\PY{n}{contourf}\PY{p}{(}\PY{n}{xx}\PY{p}{,} \PY{n}{yy}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y}\PY{p}{,} \PY{n}{edgecolors}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SVM with RBF Kernel}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

\PY{n}{plot\PYZus{}decision\PYZus{}boundary}\PY{p}{(}\PY{n}{svm\PYZus{}rbf}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy of SVM with RBF kernel: 1.00
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nb-lesson-3-2_files/nb-lesson-3-2_68_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    In this example, we use the \textbf{moons dataset}, which consists of
two interleaving half-circle shapes that cannot be separated by a
straight line. We train an SVM with an RBF kernel and then visualize the
decision boundary. The model successfully captures the complex pattern
in the data, creating a curved boundary that effectively separates the
two classes.

The RBF kernel is one of the most effective tools for handling nonlinear
data in SVM. By mapping data points into a higher-dimensional space
based on their relative distances, it allows SVM to learn complex
decision boundaries. The choice of the $ \gamma $ parameter is
crucial, as it determines the sensitivity of the model to individual
data points. When used correctly, the RBF kernel enables SVM to perform
well on a wide range of classification tasks, even when the data
exhibits intricate patterns that cannot be captured by a simple linear
model.

    One of the most remarkable properties of the \textbf{Radial Basis
Function (RBF) kernel} is that it implicitly maps the input data into an
\textbf{infinite-dimensional feature space}. This is one of the key
reasons why it is so powerful and widely used in Support Vector Machines
(SVM). To understand why, we need to analyze the mathematical structure
of the RBF kernel and its connection to function approximation.

\paragraph{Understanding the Infinite-Dimensional
Mapping}\label{understanding-the-infinite-dimensional-mapping}

At this point the reader should know that, when working with SVM, the
idea of using a \textbf{kernel function} is to implicitly transform data
into a new space where a \textbf{linear separation} is possible. Unlike
explicit transformations (such as polynomial features, which generate
new variables explicitly), the RBF kernel \textbf{performs this
transformation implicitly}, without computing the actual coordinates in
the new space.

We re-write the definition of the RBF kernel:

$$
K(x, x') = \exp\left(-\gamma \|x - x'\|^2\right)
$$

This function measures the similarity between two points $ x $ and $
x' $ based on their Euclidean distance. However, if we attempt to
express this transformation in an explicit feature space, we would find
that it requires an \textbf{infinite number of dimensions}. The reason
for this comes from \textbf{Mercer's theorem}, which states that a
positive definite kernel corresponds to a dot product in some feature
space.

In the case of the RBF kernel, it can be shown that the corresponding
feature space consists of an \textbf{infinite sum of polynomial terms}.
In other words, using the RBF kernel is equivalent to computing all
possible polynomial transformations of the data, up to infinite degree.

To understand this, recall the Taylor series expansion of the
exponential function:

$$
e^x = \sum_{k=0}^{\infty} \frac{x^k}{k!}
$$

If we expand the RBF kernel using this series, we obtain:

$$
K(x, x') = \sum_{k=0}^{\infty} \frac{(-\gamma \|x - x'\|^2)^k}{k!}
$$

This sum represents an infinite-dimensional feature space, where each
term corresponds to a polynomial of increasing degree. This means that
the RBF kernel is not just mapping data to a finite-dimensional space
(as a polynomial kernel does) but instead to an
\textbf{infinite-dimensional Hilbert space}.

Since SVM only requires computing the dot product between transformed
points, we \textbf{never explicitly construct this infinite space}.
Instead, the kernel trick allows us to compute the result directly in
the original space, avoiding the computational burden of handling
infinite-dimensional vectors.

\paragraph{Connection to Universal Function
Approximation}\label{connection-to-universal-function-approximation}

The fact that the RBF kernel corresponds to an
\textbf{infinite-dimensional feature space} has a profound implication:
it makes the SVM with an RBF kernel a \textbf{universal function
approximator}. This means that, given enough training data and the right
hyperparameters, an SVM with an RBF kernel can approximate \textbf{any
continuous function} arbitrarily well.

This property is analogous to the \textbf{universal approximation
theorem} in neural networks, which states that a neural network with at
least one hidden layer and a nonlinear activation function can
approximate any function. The key reason why this happens in SVM with an
RBF kernel is that the infinite set of basis functions generated by the
kernel allows it to \textbf{capture any complex pattern in the data}.

More formally, the RBF kernel satisfies the \textbf{Stone-Weierstrass
theorem}, which states that a sufficiently rich set of basis functions
can approximate any continuous function. Because the RBF kernel can be
written as an infinite expansion of polynomials, it has the capacity to
approximate \textbf{any smooth decision boundary} with enough support
vectors.

In practice, this means that an SVM with an RBF kernel can
\textbf{separate data with arbitrarily complex boundaries}, as long as
the right values for hyperparameters like $ \gamma $ and $ C $ are
chosen. However, this also means that the model \textbf{can overfit} if
$ \gamma $ is set too high, because it will effectively memorize the
training points rather than generalizing.

    \subsection{Appendix}\label{appendix}

    \subsubsection{Distance Between two Parallel
Lines}\label{distance-between-two-parallel-lines}

The formula for the perpendicular distance $d$ between two parallel
lines in the form $Ax + By + C_1 = 0$ and $Ax + By + C_2 = 0$ is
given by:

$$
d = \frac{|C_2 - C_1|}{\sqrt{A^2 + B^2}}
$$

\textbf{Explanation}:

\begin{itemize}
\tightlist
\item
  $A$ and $B$ are the coefficients of $x$ and $y$ in the
  equations of the lines.
\item
  $C_1$ and $C_2$ are the constant terms in the respective
  equations.
\item
  The numerator $|C_2 - C_1|$ represents the absolute difference
  between the intercepts of the lines (after normalization).
\item
  The denominator $\sqrt{A^2 + B^2}$ is the magnitude (or norm) of the
  vector $(A, B)$, which is perpendicular to the lines.
\end{itemize}

This formula calculates the shortest distance between the two parallel
lines.

    \subsubsection{Full Derivation of Eq. 4}\label{full-derivation-of-eq.-4}

To derive equation {[}4{]} step by step, we proceed as follows:

\textbf{Step 1: Lagrangian Function}

We start with the given Lagrangian function:

$$
L = \frac{1}{2} \|\mathbf{w}\|^2 - \sum\limits_{n=1}^N \alpha_n \left[ y^{(n)} \left( \mathbf{w}^T \mathbf{x}^{(n)} + w_0 \right) -1 \right]
$$

Expanding the norm:

$$
\|\mathbf{w}\|^2 = \mathbf{w}^T \mathbf{w}
$$

Thus, we rewrite $ L $ as:

$$
L = \frac{1}{2} \mathbf{w}^T \mathbf{w} - \sum\limits_{n=1}^N \alpha_n y^{(n)} \mathbf{w}^T \mathbf{x}^{(n)} - \sum\limits_{n=1}^N \alpha_n y^{(n)} w_0 + \sum\limits_{n=1}^N \alpha_n
$$

Using equation $ \eqref{eq:7.8} $:

$$
\sum\limits_{n=1}^N \alpha_n y^{(n)} = 0
$$

This implies that the term $ - \sum\limits_\{n=1\}\^{}N \alpha_n
y\^{}\{(n)\} w_0 $ vanishes:

$$
L = \frac{1}{2} \mathbf{w}^T \mathbf{w} - \sum\limits_{n=1}^N \alpha_n y^{(n)} \mathbf{w}^T \mathbf{x}^{(n)} + \sum\limits_{n=1}^N \alpha_n
$$

\textbf{Step 2: Substituting $ \mathbf{w} $}

From equation $ \eqref{eq:7.9} $, we substitute:

$$
\mathbf{w} = \sum\limits_{n=1}^N \alpha_n y^{(n)} \mathbf{x}^{(n)}
$$

Computing $ \mathbf{w}\^{}T \mathbf{w} $:

$$
\mathbf{w}^T \mathbf{w} = \left( \sum\limits_{i=1}^N \alpha_i y^{(i)} \mathbf{x}^{(i)} \right)^T \left( \sum\limits_{j=1}^N \alpha_j y^{(j)} \mathbf{x}^{(j)} \right)
$$

Expanding the dot product:

$$
\mathbf{w}^T \mathbf{w} = \sum\limits_{i=1}^N \sum\limits_{j=1}^N \alpha_i \alpha_j y^{(i)} y^{(j)} \mathbf{x}^{(i)} \cdot \mathbf{x}^{(j)}
$$

Thus,

$$
\frac{1}{2} \mathbf{w}^T \mathbf{w} = \frac{1}{2} \sum\limits_{i=1}^N \sum\limits_{j=1}^N \alpha_i \alpha_j y^{(i)} y^{(j)} \mathbf{x}^{(i)} \cdot \mathbf{x}^{(j)}
$$

Also,

$$
\sum\limits_{n=1}^N \alpha_n y^{(n)} \mathbf{w}^T \mathbf{x}^{(n)} = \sum\limits_{n=1}^N \alpha_n y^{(n)} \left( \sum\limits_{i=1}^N \alpha_i y^{(i)} \mathbf{x}^{(i)} \right)^T \mathbf{x}^{(n)}
$$

Expanding,

$$
\sum\limits_{n=1}^N \alpha_n y^{(n)} \sum\limits_{i=1}^N \alpha_i y^{(i)} \mathbf{x}^{(i)} \cdot \mathbf{x}^{(n)}
$$

Rearranging sums:

$$
\sum\limits_{i=1}^N \sum\limits_{n=1}^N \alpha_i \alpha_n y^{(i)} y^{(n)} \mathbf{x}^{(i)} \cdot \mathbf{x}^{(n)}
$$

Since this is symmetric in indices $ i $ and $ n $, it is the same
double sum as above.

\textbf{Step 3: Final Expression}

Substituting into $ L $:

$$
L = \sum\limits_{n=1}^N \alpha_n - \frac{1}{2} \sum\limits_{i=1}^N \sum\limits_{j=1}^N \alpha_i \alpha_j y^{(i)} y^{(j)} \mathbf{x}^{(i)} \cdot \mathbf{x}^{(j)}
$$

Thus, we have derived the final result:

$$
L = \sum\limits_{n=1}^N \alpha_n - \frac{1}{2} \sum\limits_{i=1}^N \sum\limits_{j=1}^N \alpha_i \alpha_j y^{(i)} y^{(j)} {\mathbf{x}^{(i)}}^T \mathbf{x}^{(j)}
$$

    \subsection{Reference and Credits}\label{reference-and-credits}

    The fundamental paper about kernel method:

\begin{itemize}
\tightlist
\item
  Aizerman, M.A., Braverman, E.M. and Rozonoer, L.I. (1964)
  ``\textbf{\emph{Theoretical Foundations of Potential Function Method
  in Pattern Recognition}}''. Automation and Remote Control, 25,
  917-936.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
