{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/polyhedron-gdl/unibo-intensive-program-2023/blob/main/2022/1-notebooks/chapter-1-3.ipynb\">\n",
    "        <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting and Overfitting \n",
    "\n",
    "When data is used for forecasting there is a danger that the machine learning model will work very well for data, but will not generalize well to other data. An obvious point is that it is important that the data used in a machine learning model be representative of the situations to which the model is to be applied. It is also important to test a model out-of-sample, by this we mean that the model should be tested on data that is different from the sample data used to determine the parameters of the model.\n",
    "\n",
    "Data scientist refer to the sample data as the **training set** and the data used to determine the accuracy of the model as the **test set**, often a **validation set** is used as well as we explain later;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# -----> load salary_vs_age_1.csv\n",
    "#\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    path = ''\n",
    "else:\n",
    "    path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Pandas libraries with alias 'pd' \n",
    "import pandas as pd \n",
    "# Read data from file 'salary_vs_age_1.csv' \n",
    "# (in the same directory that your python process is based)\n",
    "# Control delimiters, with read_table \n",
    "df1 = pd.read_table(path + \"salary_vs_age_1.csv\", sep=\";\") \n",
    "# Preview the first 5 lines of the loaded data \n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "#ax=plt.gca()\n",
    "\n",
    "df1.plot(x ='Age', y='Salary', kind = 'scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[polynomial fitting with pandas](https://joshualoong.com/2018/10/03/Fitting-Polynomial-Regressions-in-Python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x1 = df1['Age']\n",
    "y1 = df1['Salary']\n",
    "\n",
    "n = len(x1)\n",
    "\n",
    "degree = 5\n",
    "\n",
    "weights = np.polyfit(x1, y1, degree)\n",
    "model   = np.poly1d(weights)\n",
    "\n",
    "xx1 = np.arange(x1[0], x1[n-1], 0.1)\n",
    "plt.plot(xx1, model(xx1))\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Salary\")\n",
    "plt.scatter(x1,y1, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1  = np.array(y1)\n",
    "yy1 = np.array(model(x1))\n",
    "\n",
    "rmse = np.sqrt(np.sum((y1-yy1)**2)/(n-1)) \n",
    "\n",
    "print('Root Mean Square Error:')\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = []\n",
    "y_plot = [];\n",
    "\n",
    "for degree in range(1, 11):\n",
    "    weights = np.polyfit(x1, y1, degree)\n",
    "    model   = np.poly1d(weights)\n",
    "    y1      = np.array(y1)\n",
    "    yy1 = np.array(model(x1))\n",
    "\n",
    "    rmse = np.sqrt(np.sum((y1-yy1)**2)/(n-1)) \n",
    "    print(degree, rmse)\n",
    "    x_plot.append(degree)\n",
    "    y_plot.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.array(x_plot)\n",
    "y_plot = np.array(y_plot)\n",
    "\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# -----> load salary_vs_age_2.csv\n",
    "#\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    path = ''\n",
    "else:\n",
    "    path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_table(path + \"salary_vs_age_2.csv\", sep=\";\") \n",
    "x2 = df2['Age']\n",
    "y2 = df2['Salary']\n",
    "n  = len(x2)\n",
    "\n",
    "y2  = np.array(y2)\n",
    "yy2 = np.array(model(x2))\n",
    "\n",
    "rmse = np.sqrt(np.sum((y2-yy2)**2)/(n-1)) \n",
    "\n",
    "print('Root Mean Square Error:')\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx2 = np.arange(x2[0], x2[n-1], 0.1)\n",
    "plt.plot(xx2, model(xx2))\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Salary\")\n",
    "plt.scatter(x2,y2, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The root mean squared error (rmse) for the training      data set is \\$12,902\n",
    "- The rmse for the test data set is \\$38,794\n",
    "\n",
    "We conclude that the model overfits the data. The complexity of the model should be increased only until out-of-sample tests indicate that it does not generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance\n",
    "\n",
    "Suppose there is a relationship between an independent variable $x$ and a dependent variable $y$:\n",
    "\n",
    "\\begin{equation}\n",
    "    y=f(x) + \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\epsilon$ is an error term with mean zero and variance $\\sigma^2$. The error term captures either genuine randomness in the data or noise due to measurement error.\n",
    "\n",
    "Suppose we find a deterministic model for this relationship:\n",
    "\n",
    "\\begin{equation}\n",
    "    y = \\hat f(x)\n",
    "\\end{equation}\n",
    "\n",
    "Now it comes a new data point $x^\\prime$ not in the training set and we want to predict the corresponding $y^\\prime$. The error we will observe in our model at point $x^\\prime$ is going to be\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat f(x^\\prime) - f(x^\\prime) - \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "There are two different sources of error in this equation. The first one is included in the factor $\\epsilon$, the second one, more interesting, is due to what is in our training set. A robust model should give us the same prediction whatever data we used for training out model. Let's look at the average error:\n",
    "\n",
    "\\begin{equation}\n",
    "E \\left[ \\hat f (x^\\prime ) \\right] - f(x^\\prime)\n",
    "\\end{equation}\n",
    "\n",
    "where the expectation is taken over random samples of training data (having the same distributio as the training data). \n",
    "\n",
    "This is the definition of the **bias**\n",
    "\n",
    "\\begin{equation}\n",
    "    \\textrm{Bias} \\left[\\hat f (x^\\prime) \\right] = E \\left[ \\hat f (x^\\prime ) \\right] - f(x^\\prime)\n",
    "\\end{equation}\n",
    "\n",
    "We can also look at the mean square error\n",
    "\n",
    "\\begin{equation}\n",
    "E \\left[\\left( \\hat f (x^\\prime ) - f(x^\\prime) - \\epsilon \\right)^2\\right] =\n",
    "\\left[ \\textrm{Bias} \\left( \\hat f(x^\\prime) \\right) \\right]^2 + \\textrm{Var}\\left[ \\hat f(x^\\prime) \\right] + \\sigma^2\n",
    "\\end{equation}\n",
    "\n",
    "Where we remember that $\\hat f (x^\\prime)$ and $\\epsilon$ are independent.\n",
    "\n",
    "This show us that there are two important quantities, the **bias** and the **variance** that will affect our results and that we can control to some extent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIGURE 1.1 - A good model should have low bias and low variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](./pic/chapter-2-2-pic_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bias is how far away the trained model is from the correct result on average**. Where *on average* means over many goes at training the model using different data. And **Variance is a measure of the magnitude of that error**.\n",
    "\n",
    "\n",
    "Unfortunately, we often find that there is a trade-off between bias and variance. As one is reduced, the other is increased. This is the matter of over- and under-fitting.\n",
    "\n",
    "**Overfitting is when we train our algorithm too well on training data, perhaps having too many parameters for fitting**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "<div>\n",
    "<img src=\"bias_and_variance_1.png\" width=\"600\"/>\n",
    "</div>\n",
    "-->\n",
    "![caption](./pic/chapter-2-2-pic_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the scikit-learn estimator API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn is the most popular machine learning package in the data science community. Written in Python programming language, scikit-learn provides quite effective and easy to use tools for data processing to implementing machine learning models. Besides its huge adoption in the machine learning word, it continues to inspire packages like Keras and others with its now industry standard APIs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](./pic/chapter-2-2_pic_23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing scikit-learn\n",
    "\n",
    "After you’ve completed the python installations, you can install scikit-learn by running the following command from your terminal (or command prompt):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use conda as your package manager, then you can install it as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    conda install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can install the scikit-learn package directly from your Jupiter Notebook by putting an exclamation mark (!) in front of the commands above. That is like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    !pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplicity of the Scikit-learn API design:\n",
    "\n",
    "The single most important reason why scikit-learn is the most popular machine learning package out there is its simplicity. No matter you’re using a linear regression, random forest or support vector machine; you’re always calling the same functions and methods. Moreover, you can build end-to-end machine learning pipelines with a couple of codes. \n",
    "\n",
    "Here, we’d like to talk about a couple of apis such that you can do many of the machine learning tasks by using these. We’re talking about three basic interfaces: **estimator**, **predictor** and **transformer**.\n",
    "\n",
    "#### Estimator\n",
    "\n",
    "The **estimator** interface represents a machine learning model that needs to be trained on a set of data. Training a model is a central issue in any machine learning pipeline and hence we need to use this a lot. **In scikit-learn, any model can be trained easily with the fit() method of the estimator interface**. Yes, all the models regardless of regression or classification problem; supervised or unsupervised task. This is where scikit-learn’s design shines in.\n",
    "\n",
    "#### Predictor\n",
    "\n",
    "Similar to the estimator interface, there’s another one which is called the **predictor** interface. It expands the concept of an estimator by adding a **predict()** method and it represents a trained model. Once we have a trained model, most often than not we want to get predictions out of it and here it suffices to use the predict() method! The graph below demonstrates a machine learning pipeline where fit and predict methods come into play. Note that, instead of calling fit() and predict() separately, one can also use fit_predict() method which first train a model and then get the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](./pic/chapter-2-2_pic_21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "\n",
    "The next interface we want to bring to your attention is the **transformer** interface. A crucial work when working with data is to transform the variables. Whether it refers to scaling a variable or vectorizing a sentence, the transformer interface enables us to do all the transformations by calling the transform() method. Usually, we use this method after the fit() method. This is because operations that are used to transform variables are also treated as estimators. Hence, calling fit() method returns an estimator trained on a data and applying transform() on a data using this estimator transforms the data. Instead of calling fit and transform methods separately, one can also use the fit_transform() method as a short-cut. The combined fit_transform method usually works more efficiently with respect to the computation time. The figure below illustrates the usage of transform in a machine learning pipeline setting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](./pic/chapter-2-2_pic_22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning a dataset into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    path = ''\n",
    "else:\n",
    "    path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both features and target have already been scaled: mean = 0; SD = 1\n",
    "# See Chapter 4-0 for a descrition of this dataset\n",
    "df = pd.read_csv(path + 'Houseprice_data_scaled.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenient way to randomly partition this dataset into separate test and\n",
    "training datasets is to use the train_test_split function from scikit-learn's\n",
    "model_selection submodule.\n",
    "\n",
    ">**Model Selection : sklearn.model_selection**\n",
    ">\n",
    ">The selection process for the best machine learning models is largely an iterative process where data scientists search >for the best model and the best hyper-parameters. Scikit-learn offers us many useful utilities that are helpful in both >training, testing and model selection phases. In this module, there exists utilities like KFold, train_test_split(), >GridSearchCV and RandomizedSearchCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncol = df.shape[1]\n",
    "\n",
    "X = df.iloc[:, :ncol-1].values\n",
    "y = df.iloc[:, ncol-1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we assigned the NumPy array representation of the feature columns from 0 to ncol-1 to\n",
    "the variable X and we assigned the class labels from the last column to the variable\n",
    "y. Then, we used the train_test_split function to randomly split X and y into\n",
    "separate training and test datasets. By setting test_size=0.3, we assigned 30\n",
    "percent of the wine examples to X_test and y_test, and the remaining 70 percent\n",
    "of the examples were assigned to X_train and y_train, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting meaningful features: Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for the overfitting is that our model is too\n",
    "complex for the given training data. Common solutions to reduce the generalization\n",
    "error are as follows:\n",
    "\n",
    "- Collect more training data (Easier said that done...)\n",
    "- Introduce a penalty for complexity via regularization (see section ...)\n",
    "- Choose a simpler model with fewer parameters\n",
    "- Reduce the dimensionality of the data\n",
    "\n",
    "In the following\n",
    "sections, we will look at common ways to reduce overfitting by regularization, which leads to simpler models by\n",
    "requiring fewer parameters to be fitted to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ridge Regression \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression is a regularization technique where we change the function that is to be minimize. Reduce magnitude of regression coefficients by choosing a parameter $\\lambda$ and minimizing\n",
    "\t\t\n",
    "\\begin{equation}\n",
    "\t\t\\frac{1}{2N} \\sum\\limits_{n=1}^N \\left[h_\\theta \\left( x^{(n)} \\right) - y ^{(n)}\\right]^2\t+ \\lambda \\sum\\limits_{n=1}^N \\theta_i^2 \\notag\n",
    "\\end{equation}\n",
    "\n",
    "This change has the effect of encouraging the model to keep the weights $b_j$ as small as possibile. The Ridge regression should only be used for determining model parameters using the training set. Once the model parameters have been determined the penalty term should be removed for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Here we have to load the file 'salary_vs_age_1.csv'\n",
    "#\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    path = ''\n",
    "else:\n",
    "    path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Pandas libraries with alias 'pd' \n",
    "import pandas as pd \n",
    "# Read data from file 'salary_vs_age_1.csv' \n",
    "# (in the same directory that your python process is based)\n",
    "# Control delimiters, with read_table \n",
    "df1 = pd.read_table(path + \"salary_vs_age_1.csv\", sep=\";\") \n",
    "# Preview the first 5 lines of the loaded data \n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_titles = [\"Salary\",\"Age\"]\n",
    "df2=df1.reindex(columns=columns_titles)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Salary'] = df2['Salary']/1000 \n",
    "df2['Age2']=df2['Age']**2\n",
    "df2['Age3']=df2['Age']**3\n",
    "df2['Age4']=df2['Age']**4\n",
    "df2['Age5']=df2['Age']**5\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the z-score in Pandas using the .mean() and std() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the z-score method in Pandas using the .mean() and .std() methods\n",
    "def z_score(df):\n",
    "    # copy the dataframe\n",
    "    df_std = df.copy()\n",
    "    # apply the z-score method\n",
    "    for column in df_std.columns:\n",
    "        df_std[column] = (df_std[column] - df_std[column].mean()) / df_std[column].std()\n",
    "        \n",
    "    return df_std\n",
    "    \n",
    "# call the z_score function\n",
    "df2_standard = z_score(df2)\n",
    "df2_standard['Salary'] = df2['Salary']\n",
    "df2_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2_standard['Salary']\n",
    "X = df2_standard.drop('Salary',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the Ridge regularization method using the scikit-learn package [Scikit-learn](https://scikit-learn.org/stable/index.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Metrics : sklearn.metrics**\n",
    ">\n",
    ">Before starting to train our models and make predictions, we always consider which performance measure should best suit >for our task at hand. Scikit-learn provides access to a variety of these metrics. Accuracy, precision, recall, mean >squared errors are among the many metrics that are available in this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Linear Models : sklearn.linear_model**\n",
    ">\n",
    ">Linear models are the fundamental machine learning algorithms that is heavily used in supervised learning tasks. This >module contains a family of linear methods such that the target value is expected to be a linear combination of the >features. Among the models in this module, LinearRegression is the most common algorithm for regression tasks. Ridge, >Lasso and ElasticNet are models with regularization to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "y_pred = lr.predict(X)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', lr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "plt.scatter(X['Age'], y,  color='black')\n",
    "plt.plot(X['Age'], y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = Ridge(alpha=0.01, normalize=True) \n",
    "# higher the alpha value, more restriction on the coefficients; low alpha > more generalization,\n",
    "# in this case linear and ridge regression resembles\n",
    "rr.fit(X, y)\n",
    "\n",
    "y_pred_r = rr.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "plt.scatter(X['Age'], y,  color='black')\n",
    "plt.plot(X['Age'], y_pred_r, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coefficients\n",
    "print('Coefficients: \\n', rr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(y, y_pred_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso is short for *Least Absolute Shrinkage and Selection Operator*. It is similar to ridge regression except we minimize\n",
    "\n",
    "\\begin{equation}\n",
    "\t\t\\frac{1}{2N} \\sum\\limits_{n=1}^N \\left[h_\\theta \\left( x^{(n)} \\right) - y ^{(n)}\\right]^2 + \\lambda \\sum\\limits_{n=1}^N \\vert b_n \\vert \\notag\n",
    "\\end{equation}\n",
    "\n",
    "This function cannot be minimized analytically and so a variation on the gradient descent algorithm must be used. Lasso regression also has the effect of simplifying the model. It does this by setting the weights of unimportant features to zero. When there are a large number of features, Lasso can identify a relatively small subset of the features that form a good predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lsr = Lasso(alpha=.02, normalize=True, max_iter=1000000) \n",
    "# higher the alpha value, more restriction on the coefficients; low alpha > more generalization,\n",
    "# in this case linear and ridge regression resembles\n",
    "lsr.fit(X, y)\n",
    "\n",
    "y_pred_lsr = lsr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coefficients\n",
    "print('Coefficients: \\n', lsr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(y, y_pred_lsr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "plt.scatter(X['Age'], y,  color='black')\n",
    "plt.plot(X['Age'], y_pred_lsr, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Middle ground between Ridge and Lasso. Minimize\n",
    "\n",
    "\\begin{equation}\n",
    "\t\t\\frac{1}{2N} \\sum\\limits_{n=1}^N \\left[h_\\theta \\left( x^{(n)} \\right) - y ^{(n)}\\right]^2 + \\lambda_1 \\sum\\limits_{n=1}^N b_n^2 + \\lambda_2 \\sum\\limits_{n=1}^N \\vert b_n \\vert \\notag\n",
    "\\end{equation}\n",
    "\n",
    "In Lasso some weights are reduced to zero but others may be quite large. In Ridge, weights are small in magnitude but they are not reduced to zero. The idea underlying Elastic Net is that we may be able to get the best of both by making some weights zero while reducing the magnitude of the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# define model\n",
    "model = ElasticNet(alpha=1.0, l1_ratio=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:**\n",
    "\n",
    "Try to generate some plausible data distribution using a given function at your choice and adding some noise and plot the result using matplotlib library. Use the method `model_selection` of scikit-learn package to split the data so generated into train and test data. Test data must be 20% of entire data. Then try different form of interpolation and regularization of data. Discuss the final result.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "# Generate a random sample, here I use a cos, try with a different function    \n",
    "#    \n",
    "np.random.seed(42)\n",
    "x = np.sort(np.random.rand(100))\n",
    "y = np.cos(1.2 * x * np.pi) + (0.1 * np.random.randn(100))\n",
    "#\n",
    "# Total data is split into train and test data. Test data is 20% of entire data.\n",
    "#\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(x, y, test_size = 0.2)\n",
    "#\n",
    "# define picture size\n",
    "#\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "#\n",
    "# scatter plot of train data\n",
    "#\n",
    "plt.scatter(X_train, Y_train, color = 'k', label = 'Train data')\n",
    "#\n",
    "# scatter plot of test data\n",
    "#\n",
    "plt.scatter(X_test, Y_test, color = 'r', label = 'Test data')\n",
    "#\n",
    "# plot the \"true\" distribution\n",
    "#\n",
    "plt.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True-fit')\n",
    "#\n",
    "# add labels and legend\n",
    "#\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# numpy allow us to give one of new shape parameter as -1 (eg: (2,-1) or (-1,3) but not (-1, -1)). \n",
    "# It simply means that it is an unknown dimension and we want numpy to figure it out. And numpy \n",
    "# will figure this by looking at the 'length of the array and remaining dimensions' and making \n",
    "# sure it satisfies the above mentioned criteria. In other words giving an unknown number of rows and\n",
    "# 1 column means simply that the entire matrix is transformed in a column vector\n",
    "#\n",
    "x_train = X_train.reshape(-1,1)\n",
    "print(X_train.shape, x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "fit_degree=1\n",
    "#\n",
    "# As we have seen, the sklearn.preprocessing package provides several common utility functions \n",
    "# and transformer classes to change raw feature vectors into a representation that is more suitable \n",
    "# for the downstream estimators. In particular the module PolynomialFeatures generate a new feature \n",
    "# matrix consisting of all polynomial combinations of the features with degree less than or equal \n",
    "# to the specified degree. In other words it produces the same results that we have seen above in\n",
    "# paragraph 4.5.1\n",
    "#\n",
    "transf = preprocessing.PolynomialFeatures(degree = fit_degree)\n",
    "#\n",
    "# Fit the training data\n",
    "#\n",
    "x_train = transf.fit_transform(x_train)\n",
    "#\n",
    "# Choose the model (Linear Regression)\n",
    "#\n",
    "clf = LinearRegression()\n",
    "clf.fit(x_train, Y_train)\n",
    "#\n",
    "# Fit the test data\n",
    "#\n",
    "x_test = X_test.reshape(-1,1)\n",
    "transf = preprocessing.PolynomialFeatures(degree = fit_degree)\n",
    "x_test = transf.fit_transform(x_test)\n",
    "#\n",
    "# Make predictions from training data\n",
    "#\n",
    "train_predict = clf.predict(x_train)\n",
    "train_MSE = mean_squared_error(Y_train, train_predict)\n",
    "print('Training MSE:', train_MSE)\n",
    "\n",
    "test_predict = clf.predict(x_test)\n",
    "test_MSE = mean_squared_error(Y_test, test_predict)\n",
    "print('Test     MSE:', test_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model = x.reshape(-1,1)\n",
    "x_model = transf.fit_transform(x_model)\n",
    "y_model = clf.predict(x_model)\n",
    "x_test = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define picture size\n",
    "#\n",
    "fig = plt.figure(figsize = (20,10))\n",
    "#\n",
    "# Add two subplots. The subplot() function takes three arguments that describes the layout of the figure.\n",
    "# The layout is organized in rows and columns, which are represented by the first and second argument.\n",
    "# The third argument represents the index of the current plot.\n",
    "#\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "#\n",
    "# Plot of training sample\n",
    "#\n",
    "ax1.scatter(X_train, Y_train, color = 'k', label = 'Training examples')\n",
    "ax1.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "ax1.plot(x, y_model, label = 'Model function', linewidth = 3 )\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "#\n",
    "# Plot of test sample\n",
    "#\n",
    "ax2.scatter(X_test, Y_test, color = 'r', label = 'Testing examples')\n",
    "ax2.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "ax2.scatter(X_test, test_predict,color = 'k', label = 'Model predictions')\n",
    "#\n",
    "# Add legend and labels\n",
    "#\n",
    "plt.legend()\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_reg(lamda):\n",
    "    #\n",
    "    # The code should be clear since it's a simple generalization of the previous one \n",
    "    #\n",
    "    x_train = X_train.reshape(-1,1)\n",
    "    transf = preprocessing.PolynomialFeatures(degree = 20)\n",
    "    x_train = transf.fit_transform(x_train)\n",
    "    #\n",
    "    # Model choice. This time we choose Ridge model\n",
    "    #\n",
    "    clf = Ridge(alpha = lamda)\n",
    "    clf.fit(x_train, Y_train)\n",
    "    train_accuracy = clf.score(x_train, Y_train)\n",
    "    intercept = clf.intercept_\n",
    "    coefficient = clf.coef_\n",
    "    parameters = coefficient + intercept\n",
    "    x_test = X_test.reshape(-1,1)\n",
    "    transf = preprocessing.PolynomialFeatures(degree = 20)\n",
    "    x_test = transf.fit_transform(x_test)\n",
    "    test_accuracy = clf.score(x_test, Y_test)\n",
    "    train_predict = clf.predict(x_train)\n",
    "    train_MSE = mean_squared_error(Y_train, train_predict)\n",
    "    test_predict = clf.predict(x_test)\n",
    "    test_MSE = mean_squared_error(Y_test, test_predict)\n",
    "    print('Train accuracy:', train_accuracy, '\\n')\n",
    "    print('Test accuracy:', test_accuracy, '\\n')\n",
    "    print('Train MSE', train_MSE, '\\n')\n",
    "    print('Test MSE', test_MSE, '\\n')\n",
    "    print('Parameters:', parameters)\n",
    "    x_model = x.reshape(-1,1)\n",
    "    x_model = transf.fit_transform(x_model)\n",
    "    y_model = clf.predict(x_model)\n",
    "    x_test = X_test\n",
    "    fig = plt.figure(figsize = (20,10))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax1.scatter(X_train, Y_train, color = 'k', label = 'Training examples')\n",
    "    ax1.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "    ax1.plot(x, y_model, label = 'Model function', linewidth = 3 )\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax2.scatter(X_test, Y_test, color = 'r', label = 'Testing examples')\n",
    "    ax2.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "    ax2.scatter(X_test, test_predict,color = 'k', label = 'Model predictions')\n",
    "    plt.legend()\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg(0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_reg(lamda):\n",
    "    x_train = X_train.reshape(-1,1)\n",
    "    transf = preprocessing.PolynomialFeatures(degree = 20)\n",
    "    x_train = transf.fit_transform(x_train)\n",
    "    #\n",
    "    # Model choice, Lasso\n",
    "    #\n",
    "    clf = Lasso(alpha = lamda)\n",
    "    clf.fit(x_train, Y_train)\n",
    "    intercept = clf.intercept_\n",
    "    coefficient = clf.coef_\n",
    "    parameters = coefficient + intercept\n",
    "    train_accuracy = clf.score(x_train, Y_train)\n",
    "    x_test = X_test.reshape(-1,1)\n",
    "    transf = preprocessing.PolynomialFeatures(degree = 20)\n",
    "    x_test = transf.fit_transform(x_test)\n",
    "    test_accuracy = clf.score(x_test, Y_test)\n",
    "    train_predict = clf.predict(x_train)\n",
    "    train_MSE = mean_squared_error(Y_train, train_predict)\n",
    "    test_predict = clf.predict(x_test)\n",
    "    test_MSE = mean_squared_error(Y_test, test_predict)\n",
    "    print('Train accuracy:', train_accuracy, '\\n')\n",
    "    print('Test accuracy:', test_accuracy, '\\n')\n",
    "    print('Train MSE', train_MSE, '\\n')\n",
    "    print('Test MSE', test_MSE, '\\n')\n",
    "    print('Parameters:', parameters)\n",
    "    x_model = x.reshape(-1,1)\n",
    "    x_model = transf.fit_transform(x_model)\n",
    "    y_model = clf.predict(x_model)\n",
    "    x_test = X_test\n",
    "    fig = plt.figure(figsize = (20,10))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax1.scatter(X_train, Y_train, color = 'k', label = 'Training examples')\n",
    "    ax1.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "    ax1.plot(x, y_model, label = 'Model function', linewidth = 3 )\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax2.scatter(X_test, Y_test, color = 'r', label = 'Testing examples')\n",
    "    ax2.plot(x, np.cos(1.2 * x * np.pi), linewidth = 3, label = 'True function')\n",
    "    ax2.scatter(X_test, test_predict,color = 'k', label = 'Model predictions')\n",
    "    plt.legend()\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg(.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Methods for Feature Selection\n",
    "\n",
    "Ridge and Lasso regression methods, play a dual role in predictive modeling by balancing model complexity and performance. Beyond improving generalization, these techniques have significant utility in **feature selection**, a critical task in high-dimensional datasets, particularly in finance.\n",
    "\n",
    "1. **Lasso Regression**: By introducing an $L_1$ penalty on the regression coefficients, Lasso inherently performs **automatic feature selection**. This penalty encourages sparsity in the model by shrinking some coefficients to exactly zero. As a result, Lasso effectively eliminates irrelevant or redundant features, making it a powerful tool in settings where interpretability and dimensionality reduction are key priorities.\n",
    "\n",
    "2. **Ridge Regression**: Although Ridge regression imposes an $L_2$ penalty, it does not shrink coefficients to zero. Therefore, it is less effective than Lasso for direct feature elimination. However, it is instrumental in identifying and ranking the importance of correlated features, as discussed in the previous chapter.\n",
    "\n",
    "3. **Elastic Net**: A hybrid method that combines the strengths of Ridge and Lasso, Elastic Net balances feature selection and the handling of multicollinearity, making it an adaptable choice for many financial applications.\n",
    "\n",
    "When using these methods for **feature selection**, consider their behavior in the presence of high multicollinearity and the trade-offs between model sparsity and prediction accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **A. Géron**, \"*Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*\", 2nd Edition. O’Reilly Media, 2019  \n",
    "\n",
    "**S. Raschka and V. Mirjalili**, \"*Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2*\", 3rd Edition. Packt Publishing Ltd, 2019.\n",
    "     \n",
    "**S. Raschka**, \"*Model Evaluation, Model Selection and Algorithm Selection in Machine Learning*\", downlodable [here](https://arxiv.org/pdf/1811.12808.pdf)     \n",
    "     \n",
    "[Scikit-Learn web site](https://scikit-learn.org/)   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": "4",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "331px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
