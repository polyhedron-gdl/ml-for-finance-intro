{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/polyhedron-gdl/introduction-to-machine-learning-for-finance/blob/main/2022/1-notebooks/chapter-5-1.ipynb\">\n",
    "        <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Neural Network\n",
    "\n",
    "Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms. \n",
    "\n",
    "Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "<div>\n",
    "<img src='../07-pictures/03_intro_to_deep_learning_pic_0.png'  width=\"600\">\n",
    "</div>\n",
    "-->\n",
    "![caption](./pic/chapter-5-1_pic_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks rely on **training data** to learn and improve their accuracy over time. However, once these learning algorithms are fine-tuned for accuracy, they are powerful tools in computer science and artificial intelligence, allowing us to classify and cluster data at a high velocity. Tasks in speech recognition or image recognition can take minutes versus hours when compared to the manual identification by human experts. One of the most well-known neural networks is Google’s search algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mc Culloch and Pitts Artificial Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The McCulloch and Pitts neuron is one of the oldest neural network. It has a single neuron and is the simplest form of a neural network. So it is very important to learn how it works because it is the most fundamental unit of a deep neural networks. \n",
    "\n",
    "The artificial neuron receives one or more inputs and sums them to produce an output. Usually each input is separately weighted, and the sum is passed through a non-linear function known as an activation function or transfer function. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often monotonically increasing, continuous, differentiable and bounded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "<div>\n",
    "<img src='../07-pictures/1_5_intro_deep_learning_pic_0.png'  width=\"600\">\n",
    "</div>\n",
    "-->\n",
    "![caption](./pic/chapter-5-1_pic_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward neural networks, or multi-layer perceptrons (MLPs), are what we’ve primarily been focusing on within this notebook. They are comprised of an input layer, a hidden layer or layers, and an output layer. Data usually is fed into these models to train them, and they are the foundation for computer vision, natural language processing, and other neural networks. \n",
    "\n",
    "The  simplest kind of feedforward neural network is a single-layer network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node, and if the value is above some threshold the neuron fires and takes the activated value; otherwise it takes the deactivated value. \n",
    "\n",
    "In the following we are going to implement a very simple neural network from scratch without any library. In my opinion this is very usefull because most people consider neural networks as a black-box and use libraries like Keras, TensorFlow and PyTorch which provide, among other things, automatic differentiation without a real understanding of how a neural network really works. Though it is not necessary to write your own code on how to compute gradients and backprop errors, having knowledge on it helps you in understanding a few concepts which can help you a lot in understanding how a neural networks works.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hidden Layer NN\n",
    "\n",
    "We will build a shallow dense neural network with one hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](./pic/chapter-5-1_pic_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where in the graph above, we have a input vector $x = (x_1, x_2)$, containing 2 features and 4 hidden nodes $a_1, a_2, a_3$ and $a_4$, and only one value in output $y_1 \\in [0, 1]$ (consider this a binary classification task with a prediction of probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each hidden unit, take $a_1$ as example, a linear operation followed by an activation function, $f$, is performed. So given input $x = (x_1, x_2)$, inside node $a_1$, we have:\n",
    "\n",
    "$$z_1 = w_{11}x_1 + w_{12}x_2 + b_1$$\n",
    "\n",
    "$$a_1 = f(w_{11}x_1 + w_{12}x_2 + b_1) = f(z_1) $$\n",
    "\n",
    "Here $w_{11}$ denotes weight 1 of node 1, $w_{12}$ denotes weight 2 of node 1. Same for node $a_2$, it would have:\n",
    "\n",
    "$$z_2 = w_{21}x_1 + w_{22}x_2 + b_2$$\n",
    "\n",
    "$$a_2  = f(w_{21}x_1 + w_{22}x_2 + b_2) = f(z_2)$$\n",
    "\n",
    "And same for $a_3$ and $a_4$ and so on ...\n",
    "\n",
    "We can also write in a more compact form\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "z_1 \\\\ z_2 \\\\ z_3 \\\\ z_4\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "w_{11} & w_{12} \\\\ w_{21} & w_{22} \\\\ w_{31} & w_{32} \\\\ w_{41} & w_{42}\n",
    "\\end{pmatrix} \n",
    "\\cdot \n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\ x_2 \n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "b_1 \\\\ b_2 \\\\ b_3 \\\\ b_4\n",
    "\\end{pmatrix} \n",
    "\\Rightarrow Z^{[1]} = W^{[1]} \\cdot X + B^{[1]} \n",
    "\\end{equation}\n",
    "\n",
    "Note that superscript $[i]$ denotes the $ith$ layer. Let's assume that the first activation function is the $\\tanh$ and the output activation function is the $sigmoid$. So the result of the hidden layer is:\n",
    "\n",
    "$$ A^{[1]} = \\tanh{Z^{[1]}} $$\n",
    "\n",
    "This result is applied to the output node which will perform another linear operation with a different set of weights, $W^{[2]}$:\n",
    "\n",
    "$$ Z^{[2]} = W^{[2]} \\cdot A^{[1]} + B^{[2]} $$\n",
    "\n",
    "and the final output will be the result of the application of the output node activation function (the sigmoid) to this value:\n",
    "\n",
    "$$ \\hat{y} = \\sigma({Z^{[2]}})$$\n",
    "\n",
    "For the dimension of each matrix, we have:\n",
    "\n",
    "- $ W^{[1]}$ in the case above would have dimension $4 \\times 2$, with each $ith$ row is the weight of node $i$\n",
    "- $B^{[1]}$ has dimension $4 \\times 1$\n",
    "- $Z^{[1]}$ and $A^{[1]}$ both have dimention $4 \\times 1$\n",
    "- $W^{[2]}$ has dimension $1 \\times 4$\n",
    "- consequently, $Z^{[2]}$ and $A^{[2]}$ would have dimensition $1 \\times 1$, which is a single value\n",
    "\n",
    "Function $\\tanh$ and $sigmoid$ looks as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'tanh')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[10, 4])\n",
    "x = np.linspace(-10, 10)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.title('sigmoid')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, tanh(x))\n",
    "plt.title('tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the only difference of these functions is the scale of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Loss Function\n",
    " \n",
    "Remember that Linear regression uses Least Squared Error as loss function that gives a convex graph and then we can complete the optimization by finding its vertex as global minimum. However, it’s not an option for logistic regression anymore. Since the hypothesis is changed, Least Squared Error will result in a non-convex graph with local minimums by calculating with sigmoid function applied on raw model output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "<div>\n",
    "<img src='../07-pictures/3-hidden-nn.png'  width=\"600\">\n",
    "</div>\n",
    "-->\n",
    "![caption](./pic/chapter-5-1_pic_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore using logistic regression, means that we are focusing on binary classification, we have class 0 and class 1. To compare with the target, we want to constrain predictions to some values between 0 and 1. That’s why Sigmoid Function is applied on the raw model output and provides the ability to predict with probability. So will follow a different path. \n",
    "\n",
    "Intuitively, we want to assign more punishment when predicting 1 while the actual is 0 and when predict 0 while the actual is 1. The loss function of logistic regression is doing this exactly which is called Logistic Loss. If y = 1,  when prediction = 1, the cost must be = 0, otherwise, when prediction = 0, the learning algorithm is punished by a very large cost. Similarly, if y = 0, predicting 0 has no punishment but predicting 1 has a large value of cost. In formula we have\n",
    "\n",
    "\\begin{equation}\n",
    "L(y, \\hat{y}) = \n",
    "\\begin{cases} \n",
    "-\\log{\\hat{y}} & \\text{when}\\, y = 1 \\\\ -\\log(1 - \\hat{y}) & \\text{when}\\, y = 0 \n",
    "\\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "Another advantage of this loss function is that although we are looking at it by y = 1 and y = 0 separately, it can be written as one single formula which brings convenience for calculation:\n",
    "\n",
    "$$ L(y, \\hat{y}) = -[y\\log{\\hat{y}} + (1 - y)\\log{(1 - \\hat{y})}] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula of Batch Training\n",
    " \n",
    "The above shows the formula of a single input vector, however in actual training processes, a batch is trained instead of 1 at a time. The change applied in the formula is trivial, we just need to replace the single vector $x$ with a matrix $X$ with size $n \\times m$, where $n$ is number of features and $m$ is the the batch size, samples are stacked column wise, and the following result matrix are applied likewise. We have the same formulas as before ... \n",
    "\n",
    "$$ Z^{[1]} = W^{[1]}X + b^{[1]}  $$\n",
    "\n",
    "$$ A^{[1]} = \\tanh{Z^{[1]}}  $$\n",
    "\n",
    "$$ Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}  $$\n",
    "\n",
    "$$ \\hat{Y} = A^{[2]} = \\sigma({Z^{[2]}})  $$\n",
    "\n",
    "$$ J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \\frac{1}{m} \\sum_{i}^{m}L(y^{(i)}, \\hat{y}^{(i)})  $$\n",
    "\n",
    "... but for the dimension of each matrix taken in this example now we have:\n",
    "\n",
    "- $X$ has dimension $2 \\times m$, as here there are 2 features and $m$ is the batch size\n",
    "- $W^{[1]}$ in the case above would have dimension $4 \\times 2$, with each $ith$ row is the weight of node $i$\n",
    "- $b^{[1]}$ has dimension $4 \\times 1$\n",
    "- $Z^{[1]}$ and $A^{[1]}$ both have dimension $4 \\times m$\n",
    "- $W^{[2]}$ has dimension $1 \\times 4$\n",
    "- consequently, $Z^{[2]}$ and $A^{[2]}$ would have dimension $1 \\times m$\n",
    "\n",
    "Also the loss function is the same as logistic regression, but for batch training, we'll take the average loss for all training samples.\n",
    "\n",
    "This is all for the forward propagation. To activate our neurons to learn, we need to get derivative of weight parameters and update them use gradient descent.\n",
    "\n",
    "But now it is enough for us to implement the forward propagation first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Sample Dataset\n",
    " \n",
    "Scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity. Here we generate a simple **binary classification task** with 5000 data points and 20 features for later model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (4000, 20)\n",
      "test shape (1000, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Signature of make_classification\n",
    "# \n",
    "# make_classification (\n",
    "# n_samples: int=100,\n",
    "# n_features: int=20,\n",
    "# n_informative: int=2,\n",
    "# n_redundant: int=2,\n",
    "# n_repeated: int=0,\n",
    "# n_classes: int=2,\n",
    "# n_clusters_per_class: int=2,\n",
    "# weights: __class__=None,\n",
    "# flip_y: float=0.01,\n",
    "# class_sep: float=1,\n",
    "# hypercube: bool=True,\n",
    "# shift: float=0,\n",
    "# scale: float=1,\n",
    "# shuffle: bool=True,\n",
    "# random_state: __class__=None\n",
    "# )\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=5000, random_state=123)\n",
    "\n",
    "X_train, X_test = X[:4000], X[4000:]\n",
    "y_train, y_test = y[:4000], y[4000:]\n",
    "\n",
    "print('train shape', X_train.shape)\n",
    "print('test shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.72613439 -0.09447897 -0.40795222  0.89879088  0.10474933 -1.66342926\n",
      " -0.72927498  0.10438512  1.56349884  0.28581615  1.15799916 -0.15592708\n",
      "  0.82470152 -1.19743807  0.68883271 -1.50530988  0.93842871  0.38021827\n",
      " -0.66238809  0.43334625]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(X[:, 0], X[:, 1], marker='o', c=y,s=25, edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights Initialization\n",
    " \n",
    "Our neural network has 1 hidden layer and 2 layers in total(hidden layer + output layer), so there are 4 weight matrices to initialize ($W^{[1]}, b^{[1]}$ and $W^{[2]}, b^{[2]}$). Notice that the weights are initialized relatively small so that the gradients would be higher thus learning faster in the beginning phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(n_input, n_hidden, n_output):\n",
    "    params = {}\n",
    "    params['W1'] = np.random.randn(n_hidden, n_input) * 0.01\n",
    "    params['b1'] = np.zeros((n_hidden, 1))\n",
    "    params['W2'] = np.random.randn(n_output, n_hidden) * 0.01\n",
    "    params['b2'] = np.zeros((n_output, 1))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape (10, 20)\n",
      "b1 shape (10, 1)\n",
      "W2 shape (1, 10)\n",
      "b2 shape (1, 1)\n"
     ]
    }
   ],
   "source": [
    "params = init_weights(20, 10, 1)\n",
    "\n",
    "print('W1 shape', params['W1'].shape)\n",
    "print('b1 shape', params['b1'].shape)\n",
    "print('W2 shape', params['W2'].shape)\n",
    "print('b2 shape', params['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "Let's implement the forward process following equations $(5) \\sim (8)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, params):\n",
    "    \"\"\"\n",
    "    X: need to have shape (n_features x m_samples)\n",
    "    \"\"\"\n",
    "    W1, b1, W2, b2 = params['W1'], params['b1'], params['W2'], params['b2']\n",
    "    A0 = X\n",
    "    \n",
    "    cache = {}\n",
    "    Z1 = np.dot(W1, A0) + b1\n",
    "    A1 = tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache['Z1'] = Z1\n",
    "    cache['A1'] = A1\n",
    "    cache['Z2'] = Z2\n",
    "    cache['A2'] = A2\n",
    "    return  cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 shape (10, 100)\n",
      "A1 shape (10, 100)\n",
      "Z2 shape (1, 100)\n",
      "A2 shape (1, 100)\n"
     ]
    }
   ],
   "source": [
    "# get 100 samples\n",
    "inp = X[:100].T\n",
    "\n",
    "cache = forward(inp, params)\n",
    "\n",
    "print('Z1 shape', cache['Z1'].shape)\n",
    "print('A1 shape', cache['A1'].shape)\n",
    "print('Z2 shape', cache['Z2'].shape)\n",
    "print('A2 shape', cache['A2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "Following equation $(9)$, let's calculate the loss of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(Y, Y_hat):\n",
    "    \"\"\"\n",
    "    Y: vector of true value\n",
    "    Y_hat: vector of predicted value\n",
    "    \"\"\"\n",
    "    assert Y.shape[0] == 1\n",
    "    assert Y.shape == Y_hat.shape\n",
    "    m = Y.shape[1]\n",
    "    s = Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat)\n",
    "    loss = -np.sum(s) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.8650281230569485\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([np.random.choice([0, 1]) for i in range(10)]).reshape(1, -1)\n",
    "Y_hat = np.random.uniform(0, 1, 10).reshape(1, -1)\n",
    "\n",
    "l = loss(Y, Y_hat)\n",
    "print(f'loss {l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Rule\n",
    "\n",
    "Now it comes to the so colled ***delta rule*** which is the key to our weights update. With *delta rule* we compute the gradient of the loss function with respect to the weights of the network for a single input–output example.\n",
    "\n",
    "Given a generic actual value $y$, we want to minimize the loss $L$, and the technic we are going to apply here is gradient descent, basically what we need to do is to apply derivative to our variables and move them slightly down to the optimum. Here we have 2 variables, $W$ and $b$, and for this example, the update formula of them would be:\n",
    "\n",
    "$$W = W - \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "$$b = b - \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "The delta rule algorithm works by computing the gradient of the loss function with respect to each weight. In order to get the derivative of our targets, chain rules would be applied:\n",
    " \n",
    " $$\\frac{\\partial L}{\\partial W} =  \\frac{\\partial L}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial Z} \\frac{\\partial Z}{\\partial W} $$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} =  \\frac{\\partial L}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial Z} \\frac{\\partial Z}{\\partial b} $$\n",
    "\n",
    "Let's calculate ...\n",
    "\n",
    "#### Gradient Calculation\n",
    "\n",
    "$$L(y, \\hat{y}) = -[y\\log{\\hat{y}} + (1 - y)\\log{(1 - \\hat{y})}] \\Rightarrow \n",
    "\\frac{\\partial L}{\\partial \\hat y} = -\\frac{y}{\\hat y} + \\frac{1-y}{1-\\hat y} = \\frac{\\hat y - y}{\\hat y(1 - \\hat y)}$$\n",
    "\n",
    "$$ Z = W^{[i]} \\cdot x^{[i]} + B^{[i]} \\Rightarrow \\frac{\\partial Z}{\\partial W} = x \\quad \\frac{\\partial Z}{\\partial b} = 1$$\n",
    "\n",
    "\n",
    "We have to compute the derivatives of the activation functions (see appendix for details):\n",
    "\n",
    "**Hidden Layer Activation Function (Hyperbolic Tangent)**\n",
    "\n",
    "\\begin{equation}\n",
    "\\tanh x = \\frac{{{e^x} – {e^{ – x}}}}{{{e^x} + {e^{ – x}}}} \n",
    "\\Rightarrow \n",
    "\\frac{d}{{dx}}\\tanh x =  1 - {\\left(\\tanh x \\right)}^2 \n",
    "\\end{equation}\n",
    "\n",
    "**Output Layer Activation Function (Sigmoid Function)** \n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(x) =  \\left[ \\dfrac{1}{1 + e^{-x}} \\right]  \\Rightarrow\n",
    "\\dfrac{d}{dx} \\sigma(x) =  \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{equation}\n",
    "\n",
    "Given the loss function $L$ we defined above, we have gradients as follows:\n",
    "\n",
    "**Output Layer**\n",
    "\n",
    "\\begin{align}\n",
    "& \\frac{\\partial L}{\\partial \\hat y} = \\frac{\\hat y - y}{\\hat y(1 - \\hat y)} \\notag\\\\\n",
    "& \\frac{\\partial Z}{\\partial W_O} = x \\notag\\\\\n",
    "& \\frac{\\partial \\hat y}{\\partial Z} = \\frac{\\partial \\sigma}{\\partial Z} = \\sigma(Z) \\cdot (1 - \\sigma(Z)) = (\\hat y) (1 - \\hat y)\n",
    "\\end{align}\n",
    "\n",
    "So the complete gradient is:\n",
    "\n",
    "\\begin{align}\n",
    "& \\frac{\\partial L}{\\partial W_O} =  \\frac{\\partial L}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial Z} \\frac{\\partial Z}{\\partial W_O} = (\\hat y - y) \\cdot x \\notag\\\\\n",
    "& \\frac{\\partial L}{\\partial b} =  (\\hat y - y)\n",
    "\\end{align}\n",
    "\n",
    "**Hidden Layer**\n",
    "\n",
    "Now we have to calculate\n",
    "\n",
    "$$\\frac{\\partial Z}{\\partial W_H}$$\n",
    "\n",
    "Remember that\n",
    "\n",
    "$$Z = W_O \\cdot tanh\\left( W_H \\cdot X + b_H \\right) + b_O$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{\\partial Z}{\\partial W_H} = W_O \\cdot \\frac{\\partial \\, tanh(\\dots)}{\\partial W_H} \\cdot X = W_O \\cdot \\left( 1 - tanh^2(\\dots) \\right) \\cdot X$$\n",
    "\n",
    "and finally\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W_H} = (\\hat y - y) \\cdot W_O \\cdot X \\cdot \\left( 1 - tanh^2(\\dots) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "#### Weights Update\n",
    "\n",
    "Now we can compute changes in weight matrices\n",
    "\n",
    "**Output Layer**\n",
    "\n",
    "\\begin{align}\n",
    "& dW^{[2]} = \\frac{1}{m}\\left[A^{[2]} - Y \\right]A^{[1]^T} = \\frac{1}{m}\\Delta^{[2]}A^{[1]^T} \\\\\n",
    "& db^{[2]} = \\frac{1}{m}np.sum(dZ^{[2]}, axis=1, keepdims=True) \n",
    "\\end{align}\n",
    "\n",
    "**Hidden Layer**\n",
    "\n",
    "\\begin{align}\n",
    "dW^{[1]} &= \\frac{1}{m} \\left[ A^{[2]} - Y \\right] \\cdot  X^{T} \\cdot W^{[2]T} \\cdot (1 - A^{[1]^2}) \\notag\\\\\n",
    "         &= \\frac{1}{m} \\Delta^{[2]} \\cdot W^{[2]T} \\cdot (1 - A^{[1]^2}) \\cdot  X^{T}   \\notag\\\\\n",
    "         &= \\frac{1}{m} \\Delta^{[1]} \\cdot  X^{T} \n",
    "\\end{align}\n",
    "\n",
    "\\begin{equation} \n",
    "db^{[1]} = \\frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True)  \n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align} \n",
    "& \\Delta^{[2]} = A^{[2]} - Y  \\\\\n",
    "& \\Delta^{[1]} = \\Delta^{[2]} \\cdot W^{[2]T} \\cdot (1 - A^{[1]^2})\n",
    "\\end{align}\n",
    "\n",
    "In summary:\n",
    "\n",
    "- Error is calculated between the expected outputs and the outputs forward propagated from the network.\n",
    "- These errors are then propagated backward through the network from the output layer to the hidden layer, assigning a penalty for the error and updating weights as they go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "<div>\n",
    "<img src='../07-pictures/backpropagation_pseudo_code.png'  width=\"600\">\n",
    "</div>\n",
    "-->\n",
    "![caption](./pic/chapter-5-1_pic_4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In equation $(4)$ is element-wise multiplication, and the gradient of $\\tanh{x}$ is $1 - x^2$. You can try to deduct the equation above by yourself, but I basically took it from internet.\n",
    "\n",
    "Let's break down the shape of each element, given number of each layer equals `(n_x, n_h, n_y)` and batch size equals `m`:\n",
    "\n",
    "- $A^{[2]}$, $Y$ and $dZ^{[2]}$ has shape `(n_y, m)`\n",
    "- Because $A^{[1]}$ has shape `(n_h, m)`, $dW^{[2]}$ would have shape `(n_y, n_h)`\n",
    "- $db^{[2]}$ has shape `(n_y, 1)`\n",
    "\n",
    "- Because $dZ^{[2]}$ has shape `(n_y, m)`, $W^{[2]}$ has shape`(n_y, n_h)`, $dZ^{[1]}$ would have shape `(n_h, m)`\n",
    "- In equation $(5)$, $X$ has shape `(n_x, m)`, so $dW^{[1]}$ has shape `(n_h, n_x)`\n",
    "- $db^{[1]}$ has shape `(n_h, 1)`\n",
    "\n",
    "\n",
    "Once we understand the formula, implementation should come with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(params, cache, X, Y):\n",
    "    \"\"\"\n",
    "    [From coursera deep-learning course]\n",
    "    params: we initiate above with W1, b1, W2, b2\n",
    "    cache: the intermediate caculation we saved with Z1, A1, Z2, A2\n",
    "    X: shape of (n_x, m)\n",
    "    Y: shape (n_y, m)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "\n",
    "    W1 = params['W1']\n",
    "    W2 = params['W2']\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "\n",
    "    DL2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(DL2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(DL2, axis=1, keepdims=True)\n",
    "    DL1 = np.multiply(np.dot(W2.T, DL2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(DL1, X.T)\n",
    "    db1 = (1 / m) * np.sum(DL1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Loader\n",
    "\n",
    "Now let's ensemble everything into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNN:\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.params = {}\n",
    "        self.cache = {}\n",
    "        self.grads = {}\n",
    "        \n",
    "    def compute_loss(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        Y: vector of true value\n",
    "        Y_hat: vector of predicted value\n",
    "        \"\"\"\n",
    "        assert Y.shape[0] == 1\n",
    "        assert Y.shape == Y_hat.shape\n",
    "        m = Y.shape[1]\n",
    "        s = Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat)\n",
    "        loss = -np.sum(s) / m\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.params['W1'] = np.random.randn(self.n_hidden, self.n_input) * 0.01\n",
    "        self.params['b1'] = np.zeros((self.n_hidden, 1))\n",
    "        self.params['W2'] = np.random.randn(self.n_output, self.n_hidden) * 0.01\n",
    "        self.params['b2'] = np.zeros((self.n_output, 1))\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: need to have shape (n_features x m_samples)\n",
    "        \"\"\"\n",
    "        W1, b1, W2, b2 = self.params['W1'], self.params['b1'], self.params['W2'], self.params['b2']\n",
    "        A0 = X\n",
    "\n",
    "        Z1 = np.dot(W1, A0) + b1\n",
    "        A1 = tanh(Z1)\n",
    "        Z2 = np.dot(W2, A1) + b2\n",
    "        A2 = sigmoid(Z2)\n",
    "\n",
    "        self.cache['Z1'] = Z1\n",
    "        self.cache['A1'] = A1\n",
    "        self.cache['Z2'] = Z2\n",
    "        self.cache['A2'] = A2\n",
    "     \n",
    "    \n",
    "    def backward(self, X, Y):\n",
    "        \"\"\"\n",
    "        [From coursera deep-learning course]\n",
    "        params: we initiate above with W1, b1, W2, b2\n",
    "        cache: the intermediate caculation we saved with Z1, A1, Z2, A2\n",
    "        X: shape of (n_x, m)\n",
    "        Y: shape (n_y, m)\n",
    "        \"\"\"\n",
    "\n",
    "        m = X.shape[1]\n",
    "\n",
    "        W1 = self.params['W1']\n",
    "        W2 = self.params['W2']\n",
    "        A1 = self.cache['A1']\n",
    "        A2 = self.cache['A2']\n",
    "\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "        db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "        dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "        db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        self.grads = {\"dW1\": dW1,\n",
    "                      \"db1\": db1,\n",
    "                      \"dW2\": dW2,\n",
    "                      \"db2\": db2}\n",
    "\n",
    "        \n",
    "    def get_batch_indices(self, X_train, batch_size):\n",
    "        n = X_train.shape[0]\n",
    "        indices = [range(i, i+batch_size) for i in range(0, n, batch_size)]\n",
    "        return indices\n",
    "    \n",
    "    \n",
    "    def update_weights(self, lr):\n",
    "        W1, b1, W2, b2 = self.params['W1'], self.params['b1'], self.params['W2'], self.params['b2']\n",
    "        dW1, db1, dW2, db2 = self.grads['dW1'], self.grads['db1'], self.grads['dW2'], self.grads['db2']\n",
    "        self.params['W1'] -= dW1\n",
    "        self.params['W2'] -= dW2\n",
    "        self.params['b1'] -= db1\n",
    "        self.params['b2'] -= db2\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, y_train, batch_size=32, n_iterations=100, lr=0.01):\n",
    "        self.init_weights()\n",
    "        \n",
    "        indices = self.get_batch_indices(X_train, batch_size)\n",
    "        for i in range(n_iterations):\n",
    "            for ind in indices:\n",
    "                X = X_train[ind, :].T\n",
    "                Y = y_train[ind].reshape(1, batch_size)\n",
    "                \n",
    "                self.forward(X)\n",
    "                self.backward(X, Y)\n",
    "                self.update_weights(lr)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                Y_hat = self.cache['A2']\n",
    "                loss = self.compute_loss(Y, Y_hat)\n",
    "                print(f'iteration {i}: loss {loss}')\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        W1, b1, W2, b2 = self.params['W1'], self.params['b1'], self.params['W2'], self.params['b2']\n",
    "        A0 = X\n",
    "\n",
    "        Z1 = np.dot(W1, A0) + b1\n",
    "        A1 = tanh(Z1)\n",
    "        Z2 = np.dot(W2, A1) + b2\n",
    "        A2 = sigmoid(Z2)\n",
    "\n",
    "        return A2\n",
    "\n",
    "    \n",
    "def accuracy(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    Y: vector of true value\n",
    "    Y_pred: vector of predicted value\n",
    "    \"\"\"\n",
    "    def _to_binary(x):\n",
    "        return 1 if x > .5 else 0\n",
    "\n",
    "    assert Y.shape[0] == 1\n",
    "    assert Y.shape == Y_pred.shape\n",
    "    Y_pred = np.vectorize(_to_binary)(Y_pred)\n",
    "    acc = float(np.dot(Y, Y_pred.T) + np.dot(1 - Y, 1 - Y_pred.T))/Y.size\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShallowNN(20, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.2017248644570602\n",
      "iteration 10: loss 0.08974516734306448\n",
      "iteration 20: loss 0.0888776386006345\n",
      "iteration 30: loss 0.08698792970367314\n",
      "iteration 40: loss 0.07748050345373146\n",
      "iteration 50: loss 0.06835861622436348\n",
      "iteration 60: loss 0.06803918351008888\n",
      "iteration 70: loss 0.0712591156940261\n",
      "iteration 80: loss 0.06952716093009714\n",
      "iteration 90: loss 0.06628190691856602\n",
      "iteration 100: loss 0.0666382849875245\n",
      "iteration 110: loss 0.06189103694810307\n",
      "iteration 120: loss 0.05654387398845667\n",
      "iteration 130: loss 0.05290378354751587\n",
      "iteration 140: loss 0.05028825887246872\n",
      "iteration 150: loss 0.04853212571748297\n",
      "iteration 160: loss 0.04740044981180857\n",
      "iteration 170: loss 0.04669546280830373\n",
      "iteration 180: loss 0.04637107506200645\n",
      "iteration 190: loss 0.04637167893780645\n",
      "iteration 200: loss 0.046457796621860344\n",
      "iteration 210: loss 0.046474712948281816\n",
      "iteration 220: loss 0.046418823701004204\n",
      "iteration 230: loss 0.046317908082402666\n",
      "iteration 240: loss 0.046165992175765815\n",
      "iteration 250: loss 0.04591013138774351\n",
      "iteration 260: loss 0.04543782935918416\n",
      "iteration 270: loss 0.04464452959500204\n",
      "iteration 280: loss 0.04399280955374376\n",
      "iteration 290: loss 0.04399526131001989\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=100, n_iterations=300, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 94.8%\n"
     ]
    }
   ],
   "source": [
    "y_preds = model.predict(X_test.T)\n",
    "\n",
    "acc = accuracy(y_test.reshape(1, -1), y_preds)\n",
    "print(f'accuracy: {acc*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras: the Python Deep Learning API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will present the code samples found in Chapter 2, Section 1 of **Deep Learning with Python** by François Chollet the creator of ***Keras***.\n",
    "\n",
    "***Keras*** is a high-level Deep Learning API that allows you to easily build, train, evaluate and execute all sorts of neural networks. Its documentation (or specification) is available at https://keras.io. It was developed by ***François Chollet*** as part of a research project and released as an open source project in March 2015. It quickly gained popularity owing to its ease-of-use, flexibility and beautiful design. To perform the heavy computations required by neural networks, keras-team relies on a computation backend. At the present, you can choose from three popular open source deep learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK) or Theano.\n",
    "\n",
    "The problem we are trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10 categories (0 to 9). The dataset we will use is the MNIST dataset, a classic dataset in the machine learning community, which has been around for almost as long as the field itself and has been very intensively studied. It's a set of 60,000 training images, plus 10,000 test images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of \"solving\" MNIST as the \"Hello World\" of deep learning -- it's what you do to verify that your algorithms are working as expected. As you become a machine learning practitioner, you will see MNIST come up over and over again, in scientific papers, blog posts, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "<div>\n",
    "<img src='../07-pictures/MINST_1.png'  width=\"600\">\n",
    "</div>\n",
    "-->\n",
    "![caption](./pic/chapter-5-1_pic_5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_images` and `train_labels` form the \"training set\", the data that the model will learn from. The model will then be tested on the \n",
    "\"test set\", `test_images` and `test_labels`. Our images are encoded as Numpy arrays, and the labels are simply an array of digits, ranging \n",
    "from 0 to 9. There is a one-to-one correspondence between the images and the labels.\n",
    "\n",
    "Let's have a look at the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our workflow will be as follow: first we will present our neural network with the training data, `train_images` and `train_labels`. The \n",
    "network will then learn to associate images and labels. Finally, we will ask the network to produce predictions for `test_images`, and we \n",
    "will verify if these predictions match the labels from `test_labels`.\n",
    "\n",
    "Let's build our network -- again, remember that you aren't supposed to understand everything about this example just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go through this code line by line:\n",
    "\n",
    "- The first line creates a Sequential model. This is the simplest kind of Keras model, for neural networks that are just composed of a single stack of layers, connected sequentially. This is called the sequential API.\n",
    "\n",
    "- Next, we build the first layer and add it to the model. It is ***Dense*** hidden layer with 512 neurons. It will use the ReLU activation function. Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes \n",
    "\n",
    "$$\\phi \\left( Z^{[1]} = W^{[1]} \\cdot X + B^{[1]} \\right), \\quad \\phi(z) = \\textit{ReLU}(z)$$\n",
    "\n",
    "- Finally, we add a Dense output layer with 10 neurons (one per class). Using a 10-way \"softmax\" layer means that it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes.\n",
    "\n",
    "The model’s summary() method displays all the model’s layers, including each layer’s name (which is automatically generated unless you set it when creating the layer), its output shape (None means the batch size can be anything), and its number of parameters. The summary ends with the total number of parameters, including trainable and non-trainable parameters. Here we only have trainable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Dense layers often have a lot of parameters. For example, the first hidden\n",
    "layer has 784 × 512 connection weights, plus 512 bias terms, which adds up to\n",
    "401920 parameters! This gives the model quite a lot of flexibility to fit the training\n",
    "data, but it also means that the model runs the risk of overfitting, especially when you\n",
    "do not have a lot of training data. We will come back to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our network ready for training, we need to pick three more things, as part of \"compilation\" step:\n",
    "\n",
    "* A loss function: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be \n",
    "able to steer itself in the right direction.\n",
    "* An optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function.\n",
    "* Metrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly \n",
    "classified)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see [here](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a) for a description of rmsprop and [here](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html) for crossentropy, note that for a binary classification, where the number of classes M equals 2, cross-entropy is exactly the loss function of the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before training, we will preprocess our data by reshaping it into the shape that the network expects, and scaling it so that all values are in \n",
    "the `[0, 1]` interval. Previously, our training images for instance were stored in an array of shape `(60000, 28, 28)` of type `uint8` with \n",
    "values in the `[0, 255]` interval. We transform it into a `float32` array of shape `(60000, 28 * 28)` with values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to categorically encode the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is an epoch?**\n",
    "\n",
    "An epoch is a term used in machine learning and indicates the number of passes of the entire training dataset the machine learning algorithm has completed. Datasets are usually grouped into batches (especially when the amount of data is very large). Some people use the term iteration loosely and refer to putting one batch through the model as an iteration.   \n",
    "\n",
    "If the batch size is the whole training dataset then the number of epochs is the number of iterations. For practical reasons, this is usually not the case. Many models are created with more than one epoch. The general relation where dataset size is d, number of epochs is e, number of iterations is i, and batch size is b would be d*e = i*b. \n",
    "\n",
    "Determining how many epochs a model should run to train is based on many parameters related to both the data itself and the goal of the model, and while there have been efforts to turn this process into an algorithm, often a deep understanding of the data itself is indispensable.\n",
    "\n",
    "We are now ready to train our network, which in Keras is done via a call to the `fit` method of the network: \n",
    "we \"fit\" the model to its training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.2557 - acc: 0.9269\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.1034 - acc: 0.9692\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.0683 - acc: 0.9796\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0495 - acc: 0.9852\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.0380 - acc: 0.9887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f49dc88>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two quantities are being displayed during training: the \"loss\" of the network over the training data, and the accuracy of the network over \n",
    "the training data.\n",
    "\n",
    "We quickly reach an accuracy of 0.989 (i.e. 98.9%) on the training data. Now let's check that our model performs well on the test set too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our test set accuracy turns out to be 97.8% -- that's quite a bit lower than the training set accuracy. \n",
    "This gap between training accuracy and test accuracy is an example of \"overfitting\", \n",
    "the fact that machine learning models tend to perform worse on new data than on their training data. \n",
    "\n",
    "This concludes our very first example: you just saw how we could build and a train a neural network to classify handwritten digits, in less than 20 lines of Python code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other types of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks (CNNs) are similar to feedforward networks, but they’re usually utilized for image recognition, pattern recognition, and/or computer vision. These networks harness principles from linear algebra, particularly matrix multiplication, to identify patterns within an image.\n",
    "\n",
    "Recurrent neural networks (RNNs) are identified by their feedback loops. These learning algorithms are primarily leveraged when using time-series data to make predictions about future outcomes, such as stock market predictions or sales forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix - Calculation of Activation Functions Derivatives\n",
    "\n",
    "### Derivative of the Hyperbolic Tangent\n",
    "\n",
    "$$\\frac{d}{{dx}}\\tanh x = \\frac{d}{{dx}}\\left( {\\frac{{{e^x} – {e^{ – x}}}}{{{e^x} + {e^{ – x}}}}} \\right)\n",
    "= \\frac{{\\left( {{e^x} + {e^{ – x}}} \\right)\\frac{d}{{dx}}\\left( {{e^x} – {e^{ – x}}} \\right) – \\left( {{e^x} + {e^{ – x}}} \\right)\\frac{d}{{dx}}\\left( {{e^x} – {e^{ – x}}} \\right)}}{{{{\\left( {{e^x} + {e^{ – x}}} \\right)}^2}}}$$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{gathered} \\frac{d}{{dx}}\\left( {\\tanh x} \\right) = \\frac{{\\left( {{e^x} + {e^{ – x}}} \\right)\\left( {{e^x} + {e^{ – x}}} \\right) – \\left( {{e^x} + {e^{ – x}}} \\right)\\left( {{e^x} + {e^{ – x}}} \\right)}}{{{{\\left( {{e^x} + {e^{ – x}}} \\right)}^2}}} \\\\ \\Rightarrow \\frac{d}{{dx}}\\left( {\\tanh x} \\right) = \\frac{{{{\\left( {{e^x} + {e^{ – x}}} \\right)}^2} – {{\\left( {{e^x} + {e^{ – x}}} \\right)}^2}}}{{{{\\left( {{e^x} + {e^{ – x}}} \\right)}^2}}} \\\\ \\Rightarrow \\frac{d}{{dx}}\\left( {\\tanh x} \\right) = \\frac{{\\left( {{e^{2x}} + {e^{ – 2x}} + 2{e^x}{e^{ – x}}} \\right) – \\left( {{e^{2x}} + {e^{ – 2x}} – 2{e^x}{e^{ – x}}} \\right)}}{{{{\\left( {{e^x} + {e^{ – x}}} \\right)}^2}}} \\\\ \\Rightarrow \\frac{d}{{dx}}\\left( {\\tanh x} \\right) = \\frac{{\\left( {{e^{2x}} + {e^{ – 2x}} + 2} \\right) – \\left( {{e^{2x}} + {e^{ – 2x}} – 2} \\right)}}{{{{\\left( {{e^x} + {e^{ – x}}} \\right)}^2}}} \\\\ \\Rightarrow \\frac{d}{{dx}}\\left( {\\tanh x} \\right) = \\frac{{{e^{2x}} + {e^{ – 2x}} + 2 – {e^{2x}} – {e^{ – 2x}} + 2}}{{{{\\left( {{e^x} + {e^{ – x}}} \\right)}^2}}} \\\\ \\Rightarrow \\frac{d}{{dx}}\\left( {\\tanh x} \\right) = \\frac{4}{{{{\\left( {{e^x} + {e^{ – x}}} \\right)}^2}}} \\\\ \\Rightarrow \\frac{d}{{dx}}\\left( {\\tanh x} \\right) = 1 - {\\left( {\\frac{e^x - e^{-x}}{{{e^x} + {e^{ – x}}}}} \\right)^2} = 1 - {\\left(\\tanh x \\right)}^2 \\\\ \\end{gathered}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Derivative of the Sigmoid\n",
    "\n",
    "\\begin{align*}\n",
    "\\dfrac{d}{dx} \\sigma(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\notag \\\\\n",
    "&= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\notag\\\\\n",
    "&= -(1 + e^{-x})^{-2}(-e^{-x}) \\notag\\\\\n",
    "&= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\notag\\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}}  \\notag\\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}}  \\notag\\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\notag\\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\notag\\\\\n",
    "&= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Credits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Chollet F.***, \"*Deep Learning with Python*\" Manning (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Jeremy Z.***, \"*Build a Shallow Neural Network*\" click [here](https://towardsdatascience.com/building-a-shallow-neural-network-a4e2728441e0)  for the original post."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": "10",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
